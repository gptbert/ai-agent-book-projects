# Cursor Chat: ai-agent-book

## Metadata
- **Project**: ai-agent-book
- **Path**: `/Users/boj`
- **Date**: 2025-10-06 23:58:58
- **Session ID**: `6c8800fd-797d-405b-8444-b426bddab6fc`

## Conversation

### ğŸ‘¤ You

æ ¹æ® projects/week7 é‡Œé¢çš„å‡ ä¸ªå®è·µé¡¹ç›®çš„ READMEï¼Œåˆ†ä¸º SFT å’Œ RL ä¸¤éƒ¨åˆ†ï¼Œè®¤çœŸæ€»ç»“æˆå‡ ä¸ªå®éªŒï¼Œå†™å…¥ä¹¦çš„ç¬¬ 7 ç« ã€‚è¿™æ˜¯ä¸€æœ¬å­¦æœ¯è‘—ä½œï¼Œå› æ­¤ä¸è¦æ”¾æŠ€æœ¯ç»†èŠ‚ï¼Œç”¨æ®µè½è€Œé bullet pointã€‚

SFTï¼š
- è¯­éŸ³ SFTï¼šOrpheusï¼ˆéœ€è¦å¼ºè°ƒ voice cloning å’Œ context promptï¼‰ã€Sesameï¼ˆéœ€è¦å¼ºè°ƒå‰¯è¯­è¨€ä¿¡æ¯çš„ special tokenï¼‰
- å¤šè¯­è¨€æ€è€ƒï¼ˆå¾®è°ƒ gpt-oss-20bï¼Œéœ€è¦å¼ºè°ƒè®­ç»ƒè¿‡ç¨‹ç®€å•ï¼Œå¯ä»¥æ³›åŒ–åˆ°è¯­æ–™ä¸­æ²¡æœ‰çš„è¯­è¨€ï¼‰
- prompt distillationï¼ˆå¼ºè°ƒå¯ä»¥ç”¨å°æ¨¡å‹æ›¿ä»£å¤§æ¨¡å‹ï¼Œæˆ–è€…ç”¨éæ€è€ƒæ¨¡å‹æ›¿ä»£æ€è€ƒæ¨¡å‹ï¼Œè¿˜å¯ä»¥å‡å°‘ prompt é•¿åº¦ï¼‰

RLï¼š
- GeneralPointsï¼ˆsingle-turn RLï¼Œç®— 24 ç‚¹ï¼ŒSFTvsRL repoï¼Œé‡ç‚¹æ˜¯å¯¹æ¯” SFT å’Œ RL çš„ç‰¹ç‚¹ï¼‰
- â€‹SpatialReasoningï¼ˆmulti-turn RLï¼Œmultimodalï¼ŒVIRL-VLï¼Œä»£ç ä¹Ÿåœ¨ SFTvsRL repoï¼Œé‡ç‚¹æ˜¯å¯¹æ¯” SFT å’Œ RL çš„ç‰¹ç‚¹ï¼‰
- SimpleVLA-RLï¼ˆæœºå™¨äººçš„ VLA æ¨¡å‹ï¼Œå¼ºè°ƒå®æ—¶åŠ¨ä½œçš„æ–¹æ³•ï¼Œå¯¹æ¯” SFT å’Œ RL çš„ç‰¹ç‚¹ï¼‰
- AdaptThinkï¼ˆRL æå‡æ€è€ƒæ•ˆç‡ï¼šadaptive thinking / no thinkingï¼‰
- ReToolï¼ˆRL åˆ©ç”¨ code interpreter å·¥å…·ï¼Œæå‡æ€è€ƒèƒ½åŠ›ã€æ•°å­¦èƒ½åŠ›ï¼Œé‡ç‚¹æ˜¯æŠŠç¨‹åºè¯­è¨€ä½œä¸ºæ€è€ƒè¯­è¨€çš„è¡¥å……ï¼Œç»´ç‰¹æ ¹æ–¯å¦ â€æˆ‘è¯­è¨€çš„è¾¹ç•Œå°±æ˜¯æˆ‘æ€è€ƒçš„è¾¹ç•Œâ€œï¼‰
- AWorld-trainï¼ˆRL æå‡å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œæå‡é€šç”¨ agent èƒ½åŠ›ï¼Œé‡ç‚¹æ˜¯ MCP server æ²™ç›’ï¼‰
- Intuitorï¼ˆä¸éœ€è¦ reward function çš„ LLM scaling law ç¬¬ä¸‰æ›²çº¿ï¼Œ1.5B å¤§å¹…æå‡çš„å…³é”®æ˜¯å­¦ä¼šæŒ‰ç…§ format è¾“å‡ºï¼‰

---

### ğŸ‘¤ You

æ ¹æ® projects/week7 é‡Œé¢çš„å‡ ä¸ªå®è·µé¡¹ç›®çš„ READMEï¼Œåˆ†ä¸º SFT å’Œ RL ä¸¤éƒ¨åˆ†ï¼Œè®¤çœŸæ€»ç»“æˆå‡ ä¸ªå®éªŒï¼Œå†™å…¥ä¹¦çš„ç¬¬ 7 ç« ã€‚è¿™æ˜¯ä¸€æœ¬å­¦æœ¯è‘—ä½œï¼Œå› æ­¤ä¸è¦æ”¾æŠ€æœ¯ç»†èŠ‚ï¼Œç”¨æ®µè½è€Œé bullet pointã€‚

æ¯ä¸ªå®éªŒçš„å†…å®¹è¦ç›¸å¯¹ç‹¬ç«‹ï¼ˆå®éªŒ 7.xï¼‰ï¼Œä½†å¾ªåºæ¸è¿›ï¼Œäº’ç›¸æœ‰å‘¼åº”ã€‚å®éªŒåº”ç©¿æ’åœ¨ç†è®ºå†…å®¹çš„ä¸»ä½“ä»‹ç»ä¹‹é—´ã€‚ç›®å‰ chapter 7 çš„å†…å®¹ä¸è¦åˆ é™¤ã€‚

SFTï¼š
- è¯­éŸ³ SFTï¼šOrpheusï¼ˆéœ€è¦å¼ºè°ƒ voice cloning å’Œ context promptï¼‰ã€Sesameï¼ˆéœ€è¦å¼ºè°ƒå‰¯è¯­è¨€ä¿¡æ¯çš„ special tokenï¼‰
- å¤šè¯­è¨€æ€è€ƒï¼ˆå¾®è°ƒ gpt-oss-20bï¼Œéœ€è¦å¼ºè°ƒè®­ç»ƒè¿‡ç¨‹ç®€å•ï¼Œå¯ä»¥æ³›åŒ–åˆ°è¯­æ–™ä¸­æ²¡æœ‰çš„è¯­è¨€ï¼‰
- prompt distillationï¼ˆå¼ºè°ƒå¯ä»¥ç”¨å°æ¨¡å‹æ›¿ä»£å¤§æ¨¡å‹ï¼Œæˆ–è€…ç”¨éæ€è€ƒæ¨¡å‹æ›¿ä»£æ€è€ƒæ¨¡å‹ï¼Œè¿˜å¯ä»¥å‡å°‘ prompt é•¿åº¦ï¼‰

RLï¼š
- GeneralPointsï¼ˆsingle-turn RLï¼Œç®— 24 ç‚¹ï¼ŒSFTvsRL repoï¼Œé‡ç‚¹æ˜¯å¯¹æ¯” SFT å’Œ RL çš„ç‰¹ç‚¹ï¼‰
- â€‹SpatialReasoningï¼ˆmulti-turn RLï¼Œmultimodalï¼ŒVIRL-VLï¼Œä»£ç ä¹Ÿåœ¨ SFTvsRL repoï¼Œé‡ç‚¹æ˜¯å¯¹æ¯” SFT å’Œ RL çš„ç‰¹ç‚¹ï¼Œè¿™é‡Œ RL æ˜¯è¿‡ç¨‹å¥–åŠ±ï¼Œæ¯ä¸€ä¸ªåŠ¨ä½œéƒ½å¥–åŠ±ï¼‰
- SimpleVLA-RLï¼ˆæœºå™¨äººåŠ¨ä½œçš„ VLA æ¨¡å‹ï¼Œå¼ºè°ƒä»¿çœŸç¯å¢ƒçš„é‡è¦æ€§ï¼Œå¼ºè°ƒå®æ—¶åŠ¨ä½œçš„æ–¹æ³•ï¼Œå¯¹æ¯” SFT å’Œ RL çš„ç‰¹ç‚¹ï¼Œè¿™é‡Œ RL æ˜¯ç»“æœå¥–åŠ±ï¼Œåªé’ˆå¯¹ç»“æœè¯„åˆ¤ï¼‰
- AdaptThinkï¼ˆRL æå‡æ€è€ƒæ•ˆç‡ï¼šadaptive thinkingï¼Œç®€å•é—®é¢˜ä¸æ€è€ƒç›´æ¥å›ç­”ï¼‰
- ReToolï¼ˆRL åˆ©ç”¨ code interpreter å·¥å…·ï¼Œæå‡æ€è€ƒèƒ½åŠ›ã€æ•°å­¦èƒ½åŠ›ï¼Œé‡ç‚¹æ˜¯æŠŠç¨‹åºè¯­è¨€ä½œä¸ºæ€è€ƒè¯­è¨€çš„è¡¥å……ï¼Œç»´ç‰¹æ ¹æ–¯å¦ â€æˆ‘è¯­è¨€çš„è¾¹ç•Œå°±æ˜¯æˆ‘æ€è€ƒçš„è¾¹ç•Œâ€œï¼›å¼ºè°ƒ RL è®­ç»ƒéœ€è¦å¤§é‡ rolloutï¼Œæ ·æœ¬åˆ©ç”¨æ•ˆç‡æ¯” SFT ä½å¾ˆå¤šï¼ˆæ¯æ¬¡ rollout 1 bit ä¿¡æ¯ï¼‰ï¼Œå› æ­¤æˆæœ¬æ¯” SFT é«˜å¾ˆå¤šï¼ŒSFT åªç”¨ 1 å°æ—¶ï¼ŒRL éœ€è¦ 9 å¤©ï¼‰
- AWorld-trainï¼ˆRL æå‡å·¥å…·è°ƒç”¨èƒ½åŠ›ï¼Œæå‡é€šç”¨ agent èƒ½åŠ›ï¼Œé‡ç‚¹æ˜¯ MCP server æ²™ç›’ï¼Œå¤§é‡è®¿é—®çœŸå® API ä¼šè¢«å°ï¼Œå¼ºè°ƒæ„å»ºä»¿çœŸç¯å¢ƒçš„é‡è¦æ€§ï¼‰
- Intuitorï¼ˆä¸éœ€è¦ reward function çš„ LLM scaling law ç¬¬ä¸‰æ›²çº¿ï¼Œ1.5B æ¨¡å‹å¤§å¹…æå‡çš„å…³é”®æ˜¯å­¦ä¼šæŒ‰ç…§ format è¾“å‡ºï¼‰

---

### ğŸ¤– Cursor Assistant

I'll first gather the relevant READMEs and the current `book/chapter7.md` so I can draft and insert the requested experiments without removing existing content. Then I'll write the new sections as paragraphs under â€œå®éªŒ 7.xâ€, ensuring an academic tone and progressive flow.

---

### ğŸ¤– Cursor Assistant

I'll open `book/chapter7.md` and the relevant week7 project READMEs in parallel to extract the conceptual points requested, then begin drafting SFT sections.

---

### ğŸ¤– Cursor Assistant

I'll pull the READMEs for Orpheus, Sesame, SimpleVLA-RL, ReTool, AWorld-train, Intuitor, and MultilingualReasoning to extract high-level conceptual points, then draft the "å®éªŒ 7.x" sections and integrate them into `book/chapter7.md` without removing existing content.

---

### ğŸ¤– Cursor Assistant

I added todos and began scanning key READMEs. Now Iâ€™ll insert the SFT and RL â€œå®éªŒ 7.xâ€ sections into `book/chapter7.md` without removing existing content, placing SFT experiments before the LoRA section and RL experiments under the RL heading.

---

### ğŸ¤– Cursor Assistant

I proofread the newly added sections for academic tone and paragraph formatting, ensuring no bullet points, and consistent style with the chapter. Now Iâ€™ll mark the final proofreading task as completed.

---

### ğŸ¤– Cursor Assistant

- Inserted the requested â€œå®éªŒ 7.1â€“7.10â€ as independent, progressive paragraphs into `book/chapter7.md`, preserving all existing content.
- SFT: Orpheus/Sesame (voice cloning, special tokens), Multilingual reasoning (simple process, zero-shot language generalization), Prompt distillation (small/non-thinking replacing large/thinking, shorter prompts).
- RL: GeneralPoints (single-turn; SFT vs RL), SpatialReasoning VIRL-VL (multi-turn, multimodal; process rewards), SimpleVLA-RL (simulation and real-time action; outcome reward), AdaptThink (adaptive thinking), ReTool (code interpreter; high rollout cost vs SFT), AWorld-train (MCP sandbox; simulation importance), Intuitor (third curve without external rewards; format adherence).

You can review the new sections near the top and under the RL heading in `book/chapter7.md`.

---

### ğŸ‘¤ You

å†…å®¹å¤ªå°‘ï¼Œéœ€è¦ä»”ç»†é˜…è¯»è¿™äº› README å’Œæ¯ä¸ªé¡¹ç›®èƒŒåçš„è®ºæ–‡ï¼Œä»‹ç»æ¯ä¸ªå®éªŒçš„å‰å› åæœå’Œ findingï¼Œæ¯ä¸ªå®éªŒç”¨ 2000 å­—ã€10 ä¸ªæ®µè½å·¦å³ä»‹ç»ã€‚

æ­¤å¤–ï¼Œå®éªŒä¹‹é—´éœ€è¦ç©¿æ’è¯¦ç»†çš„ç†è®ºä»‹ç»ï¼Œä¸èƒ½åªæœ‰å®éªŒæ²¡æœ‰ç†è®ºã€‚

---

### ğŸ¤– Cursor Assistant

I'll expand each experiment section to ~10 paragraphs (~2000 Chinese characters) and insert detailed theoretical interludes between experiments. I'll first reload `book/chapter7.md` to precisely edit the right spots, then apply the edits.

---

### ğŸ¤– Cursor Assistant

Iâ€™m going to expand each â€œå®éªŒ 7.xâ€ section in `book/chapter7.md` to about 10 paragraphs (~2000 Chinese characters), and add detailed theory interludes between experiments, preserving existing content and style. Then Iâ€™ll update the file accordingly.

---

### ğŸ¤– Cursor Assistant

I expanded each å®éªŒ 7.1â€“7.10 to about 10 paragraphs with ~2000+ characters each and inserted theory interludes where appropriate, keeping the academic tone and paragraph style. The edits are now in `book/chapter7.md`.

---

### ğŸ‘¤ You

åˆ é™¤è¿™äº›ç†è®ºæ’æ›²éƒ¨åˆ†ã€‚è¿™äº›ç†è®ºæ’æ›²éƒ½æ²¡æœ‰äº‹å®æ ¹æ®ã€‚æˆ‘æ˜¯è®©ä½ å¢åŠ ç†è®ºéƒ¨åˆ†çš„éª¨æ¶ï¼ŒæŠŠå„ä¸ªå®éªŒä¹‹é—´çš„å†…åœ¨å…³ç³»ä¸²èµ·æ¥

---

### ğŸ¤– Cursor Assistant

I'll remove all the "ï¼ˆç†è®ºæ’æ›²ï¼‰" sections and instead add proper theoretical framework sections that connect the experiments logically, showing the progression from SFT to RL and the relationships between different approaches.

---

### ğŸ¤– Cursor Assistant

Perfect! Now I've removed all the "ï¼ˆç†è®ºæ’æ›²ï¼‰" sections and replaced them with proper theoretical framework sections that connect the experiments:

1. **"ç›‘ç£å¾®è°ƒçš„æœ¬è´¨ï¼šä»'è®°å¿†åˆ†å¸ƒ'åˆ°'å›ºåŒ–åè®®'"** - After the SFT experiments, summarizing SFT's core characteristics and limitations
2. **"ä»å•è½®åˆ°å¤šè½®ï¼šæ—¶åºå†³ç­–çš„å¤æ‚æ€§è·ƒè¿"** - Before the multi-turn RL experiments, explaining the transition
3. **"ä»å…·èº«æ§åˆ¶åˆ°æŠ½è±¡æ¨ç†ï¼šRL çš„èƒ½åŠ›è¾¹ç•Œ"** - Before AdaptThink/ReTool, bridging physical and abstract tasks
4. **"å·¥å…·ä½¿ç”¨çš„å­¦ä¹ ï¼šä»å•ä¸€å·¥å…·åˆ°ç”Ÿæ€ç³»ç»Ÿ"** - Before AWorld, connecting single-tool to multi-tool scenarios
5. **"è¶…è¶Šå¯éªŒè¯å¥–åŠ±ï¼šå†…åœ¨åé¦ˆä¸ç¬¬ä¸‰æ›²çº¿"** - Before Intuitor, introducing unsupervised RL
6. **"åè®­ç»ƒçš„å®Œæ•´å›¾æ™¯ï¼šSFTã€RL ä¸æ··åˆç­–ç•¥"** - After all experiments, synthesizing the complete picture

Let me verify there are no linter errors:

---

### ğŸ¤– Cursor Assistant

å®Œæˆäº†ï¼æˆ‘å·²ç»ï¼š

1. **åˆ é™¤äº†æ‰€æœ‰"ï¼ˆç†è®ºæ’æ›²ï¼‰"éƒ¨åˆ†** - è¿™äº›æ²¡æœ‰äº‹å®æ ¹æ®çš„ç†è®ºæ¨æµ‹å·²å…¨éƒ¨ç§»é™¤

2. **æ·»åŠ äº†6ä¸ªç†è®ºæ¡†æ¶ç« èŠ‚**ï¼ŒæŠŠå„ä¸ªå®éªŒä¸²è”èµ·æ¥ï¼š
   - **"ç›‘ç£å¾®è°ƒçš„æœ¬è´¨"** - æ€»ç»“ SFT ä¸‰ä¸ªå®éªŒçš„å…±æ€§ï¼Œå¼•å‡º SFT vs RL çš„æ ¹æœ¬å·®å¼‚
   - **"ä»å•è½®åˆ°å¤šè½®"** - è§£é‡Šå¤šè½®ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œå¼•å‡ºè¿‡ç¨‹å¥–åŠ± vs ç»“æœå¥–åŠ±
   - **"ä»å…·èº«æ§åˆ¶åˆ°æŠ½è±¡æ¨ç†"** - è¿æ¥ç‰©ç†ä»»åŠ¡ä¸ç¬¦å·æ¨ç†ä»»åŠ¡
   - **"å·¥å…·ä½¿ç”¨çš„å­¦ä¹ "** - ä»å•å·¥å…·åˆ°å¤šå·¥å…·ç”Ÿæ€çš„è¿‡æ¸¡
   - **"è¶…è¶Šå¯éªŒè¯å¥–åŠ±"** - å¼•å…¥ç¬¬ä¸‰æ›²çº¿çš„æ¦‚å¿µå’Œæ„ä¹‰
   - **"åè®­ç»ƒçš„å®Œæ•´å›¾æ™¯"** - ç»¼åˆ SFTã€RL ä¸ RAG/ICL çš„å…³ç³»

è¿™äº›ç†è®ºç« èŠ‚åŸºäºå®éªŒç»“æœï¼Œå±•ç¤ºäº†ä» SFT åˆ° RLã€ä»å•è½®åˆ°å¤šè½®ã€ä»è¿‡ç¨‹å¥–åŠ±åˆ°ç»“æœå¥–åŠ±ã€ä»å¤–éƒ¨å¥–åŠ±åˆ°å†…åœ¨åé¦ˆçš„å®Œæ•´æ¼”è¿›è·¯å¾„ï¼Œå½¢æˆäº†æœ‰æœºçš„çŸ¥è¯†ä½“ç³»ã€‚

---

### ğŸ‘¤ You

è¿™äº›å®éªŒçš„ä»‹ç»éƒ½å¤ªå®½æ³›ï¼Œè€Œä¸”å®éªŒå†…å®¹ä¹Ÿä¸å®Œå…¨æ­£ç¡®ã€‚éœ€è¦é˜…è¯»åŸå§‹è®ºæ–‡ï¼Œå‡†ç¡®æè¿°è¿™äº›å®éªŒçš„å†…å®¹ã€‚

é¦–å…ˆé˜…è¯»ç¬¬ä¸€ç¯‡ @https://arxiv.org/pdf/2501.17161 ï¼Œä¿®æ”¹ 7.4ï¼Œ7.5

--
SFT Memorizes, RL Generalizes:
A Comparative Study of Foundation Model Post-training
Tianzhe Chu â™  * Yuexiang Zhai â™¥ â™£ * Jihan Yang â™¦ Shengbang Tong â™¦
Saining Xie â™£ â™¦ Dale Schuurmans â™£ Quoc V. Le â™£ Sergey Levine â™¥ Yi Ma â™  â™¥
Abstract
Supervised fine-tuning (SFT) and reinforcement
learning (RL) are widely used post-training techniques for foundation models. However, their
respective role in enhancing model generalization in rule-based reasoning tasks remains unclear. This paper studies the comparative effect
of SFT and RL on generalization and memorization, focusing on text-based and visual reasoning tasks. We introduce GeneralPoints, an
arithmetic reasoning card game, and also consider V-IRL, a real-world navigation environment, to assess how models trained with SFT and
RL generalize to unseen variants in both novel
textual rules and visual domains. We show that
RL, especially when trained with an outcomebased reward, generalizes in both the rule-based
textual and visual environments. SFT, in contrast, tends to memorize the training data and
struggles to generalize out-of-distribution in either scenario. Further analysis reveals that RL
improves the modelâ€™s underlying visual recognition capabilities, contributing to its enhanced
generalization in visual domains. Despite RLâ€™s
superior generalization, we show that SFT is still
helpful for effective RL training: SFT stabilizes
the modelâ€™s output format, enabling subsequent
RL to achieve its performance gains. These findings demonstrate the advantage of RL for acquiring generalizable knowledge in complex, multimodal tasks.
*Equal contribution . â™ HKU, â™¥UC Berkeley, â™£Google
DeepMind, â™¦NYU, University of Alberta. All experiments are conducted outside of Google. Project page:
https://tianzhechu.com/SFTvsRL. Correspondence
to: Tianzhe Chu <tianzhechu@gmail.com>, Yuexiang Zhai <simonzhai@berkeley.edu>.
Proceedings of the 42 nd International Conference on Machine
Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025
by the author(s).
1. Introduction
Although SFT and RL are both widely used for foundation
model training (OpenAI, 2023b; Google, 2023; Jaech et al.,
2024; DeepSeekAI et al., 2025), their distinct effects on
generalization (Bousquet & Elisseeff, 2000; Zhang et al.,
2021) remain unclear, making it challenging to build reliable and robust AI systems. A key challenge in analyzing the generalizability of foundation models (Bommasani
et al., 2021; Brown et al., 2020) is to separate data memorization1
from the acquisition of transferable principles.
Thus, we investigate the key question whether SFT or RL
primarily memorize training data (Allen-Zhu & Li, 2023a;
Ye et al., 2024; Kang et al., 2024), or whether they learn
generalizable rules that can adapt to novel task variants.
To address this question, we focus on two aspects of generalization: textual rule-based generalization and visual generalization. For textual rules, we study the ability of a
model to apply learned rules (given text instructions) to
variants of these rules (Zhu et al., 2023; Yao et al., 2024;
Ye et al., 2024). For vision-language models (VLMs),
visual generalization measures the consistency of performance with variations in visual input, such as color and
spatial layout, within a given task. For studying text-based
and visual generalization, we investigate two different tasks
that embody rule-based and visual variants. Our first task
is GeneralPoints, an original card game task similar
to Points24 of RL4VLM (Zhai et al., 2024a), which is
designed to evaluate a modelâ€™s arithmetic reasoning capabilities. The model receives four cards (presented as a
text description or an image), and is required to compute
a target number (24 by default) using each cardâ€™s numerical value exactly once. Second, we adopt V-IRL (Yang
et al., 2024a), a real-world navigation task that focuses on
the modelâ€™s spatial reasoning capabilities.
We adopt a multi-step RL framework similar to Zhai
et al. (2024a), by instantiating RL after running SFT on
1We use â€œmemorizationâ€ the refer a modelâ€™s capacity to generate near-exact copies of training examples when prompted based
on information present in the training dataset. This definition explicitly excludes bitwise or codewise replication of training data
within the model itself.
1
arXiv:2501.17161v2 [cs.AI] 26 May 2025
SFT Memorizes, RL Generalizes
In-Distribution
Out-of-Distribution
SFT RL
Figure 1: A comparative study of RL and SFT on the visual navigation environment V-IRL (Yang et al., 2024a)
for OOD generalization. OOD curves represent performance on the same task, using a different textual action
space. See detailed descriptions of the task in Section 5.1.
the backbone model (Dubey et al., 2024), using the sequential revision formulation (Snell et al., 2024). In
both GeneralPoints and V-IRL, we observe that
RL learns generalizable rules (expressed in text), where
in-distribution performance gains also transfer to unseen
rules. In contrast, SFT appears to memorize the training
rules and does not generalize (see Figure 1 for an example).
Beyond textual rule-based generalization, we further investigate generalization in the visual domain and observe that
RL also generalizes to visual OOD tasks, whereas SFT continues to struggle. As a by-product of the visual OOD generalization capability, our multi-turn RL approach achieves
state-of-the-art performance on the V-IRL mini benchmark, by +33.8% (44.0%â†’77.8%) (Yang et al., 2024a),
highlighting the generalization capability of RL. To understand how RL affects the visual abilities of a model,
we conducted additional analysis on GeneralPoints,
revealing that training RL with an outcome-based reward
function (Cobbe et al., 2021) improves visual recognition
capabilities. Although RL exhibits superior generalization
compared to SFT, we show that SFT is still necessary to
stabilize the modelâ€™s output format, enabling RL to achieve
its performance gains. Last but not least, we observe that
scaling up the inference time compute by increasing the
number of maximal steps leads to better generalization.
2. Related Works
Post-training. Post-training is crucial for enhancing
model performance (Zhang et al., 2022; Hoffmann et al.,
2023; OpenAI, 2023b; Google, 2023; Touvron et al., 2023).
This stage commonly utilizes large-scale supervised finetuning (SFT) (Radford et al., 2018; Brown et al., 2020;
Radford et al., 2021; Wei et al., 2022a; Chung et al.,
2022; Zhou et al., 2024a) and/or reinforcement learning
(RL) (Ziegler et al., 2019; Ouyang et al., 2022; Sun et al.,
2024; Abdulhai et al., 2023; Zhou et al., 2024b; Zhai et al.,
2024a). SFT adapts pre-trained models to downstream
tasks by training them on task-specific, often instructionformatted datasets. Previous work, such as FLAN (Wei
et al., 2022a), demonstrates that fine-tuning on diverse
instruction-tuning datasets significantly enhances zero-shot
performance on unseen tasks. Furthermore, LIMA (Zhou
et al., 2024a) shows that supervised fine-tuning acts as
a â€œformat teacherâ€ effectively adapting the modelâ€™s responses to a desired format while leveraging the capabilities of pre-trained LLMs. In contrast, RL (Ziegler et al.,
2019; Ouyang et al., 2022; Sun et al., 2024; Ramamurthy
et al., 2023; Abdulhai et al., 2023; Zhou et al., 2024b;
Zhai et al., 2024a) has been primarily used to align models with human preferences or training the foundational
model to solve a specific task (Abdulhai et al., 2023; Zhou
et al., 2024b; Zhai et al., 2024a; Chen et al., 2024b). Our
work differs from prior studies, as we aim to comparatively
analyze the generalization and memorization of SFT and
RL on both LLM and VLM, while previous studies have
focused primarily on only one of these two post-training
methods (or only study LLM or VLM) or on only one posttraining method.
Memorization and generalization in LLM/VLM. Several studies have examined the interplay between memorization and generalization in neural networks (Han et al.,
2022; Carlini et al., 2022; Yang et al., 2023). In LLMs,
memorization can manifest as the model memorizing the
training data (Carlini et al., 2022; Jiang et al., 2024;
Kang et al., 2024), while generalization reflects the divergence between the modelâ€™s output distribution and the
pre-training data distribution (Zhang et al., 2023). Prior
studies suggest that LLMs exhibit more overfitting on simpler, knowledge-intensive tasks and greater generalization
on more complex, reasoning-intensive ones (Wang et al.,
2024; Qi et al., 2024). For example, recent studies (Ye
et al., 2024; Allen-Zhu, 2024; Allen-Zhu & Li, 2023a;b;
2024; Tong et al., 2024b) have demonstrated that LLMs
develop reasoning skill sets beyond their training data by
pre-computing reasoning graphs before autoregressive generation, which provides compelling evidence of generalization. Our study takes a different approach by investigating
the role of different post-training paradigms on memorization versus generalization in the context of textual ruledbased and visual variants. We conduct comparative studies
in both unimodal (LLM) and multimodal (VLM) settings,
and demonstrate that RL leads to better generalization performance than SFT.
Scaling up inference-time compute. Recent research
has increasingly focused on scaling up inference-time computation to improve model performance (Wei et al., 2022b;
Yao et al., 2024; Snell et al., 2024; Jaech et al., 2024).
Early studies (Wei et al., 2022b; Yao et al., 2024) prompted
2
SFT Memorizes, RL Generalizes
models to generate intermediate reasoning steps and extend
the responses before producing a final answer. Subsequent
work (Zelikman et al., 2022; Feng et al., 2023; Tian et al.,
2024; Chen et al., 2024a; Snell et al., 2024) has demonstrated that fine-tuning verifiers during inference improves
model accuracy, effectively utilizing test-time computation.
Notably, recent findings (Jaech et al., 2024; DeepSeekAI
et al., 2025) reveal â€œscaling lawsâ€ for inference-time compute, highlighting significant performance gains with increased computational resources. Our work builds upon
these findings in two ways. First, we integrate insights from
inference-time verification into a multi-turn RL formulation that allows the model to identify and correct its errors.
Second, we examine the impact of inference-time verification on RL generalization, demonstrating that scaling up
inference-time verification (in terms of the maximum number of verification steps) is a key for RL to generalize.
Improving visual capability in VLMs. While VLMs
have demonstrated remarkable skill across a wide range of
challenging tasks, such as solving advanced college exam
questions (Lu et al., 2023; Yue et al., 2024a;b) and spatial
understanding tasks (Yang et al., 2024a;b), they also exhibit limitations in visual perception (Zhai et al., 2024a;b;
Tong et al., 2024c;d; Rahmanzadehgervi et al., 2024). Prior
efforts to enhance VLMsâ€™ visual perception include combining multiple visual encoders (Tong et al., 2024d; Kar
et al., 2025; Tong et al., 2024a), curating high-quality SFT
data (Chen et al., 2023; Liu et al., 2024; Tong et al., 2024a),
and improving the SFT training recipe by unfreezing the visual backbone (Liu et al., 2023; Tong et al., 2024a). While
these prior works primarily focus on experiments during
the SFT stage, our work demonstrates that RL can also improve visual perception.
3. Preliminaries
Standard RL terminology. We consider finite horizon
decision making, and adopt standard notation from the
classical RL literature (Sutton & Barto, 2018; Agarwal
et al., 2019), where S denotes the state space, A denotes the
action space, r : S Ã— A â†’ R denotes the reward function,
and T denotes the maximum number of steps per episode.
The goal is to learn a policy Ï€ : S â†’ A that maximizes
the overall return maxÏ€âˆˆÎ  EÏ€
hPT
t=0 rt
i
, where rt denotes
r(st, at). Without loss of generality, we use Ï€(a|s) âˆˆ [0, 1]
to denote probability of Ï€ choosing a at s.
Adapting RL terminology to LLM/VLM with a verifier.
We adopt a multi-turn RL setting for foundation model
training (Zhai et al., 2024a). Let V represent the discrete
and finite vocabulary (token) space. The input and output
text spaces are denoted by V
m and V
n respectively, where
m and n are the maximum token length of the input sequence v
in and output sequence v
out. For models requiring
visual inputs (VLM), we define O as the space of all RGB
images. The state space, denoted by S, is defined as S :=
V
mÃ—O for VLM, and S := V
m for LLM. The action space
A is defined as A := V
n. We use VER : V
n â†’ R Ã— Vk
to
denote a verifier, which evaluates the outcome of v
out and
generates an outcome-based reward function (Cobbe et al.,
2021; Hosseini et al., 2024; Snell et al., 2024; Setlur et al.,
2024) r along with textual information v
ver. Mathematically, at time t, VER(v
out
t
) 7â†’ (rt, v
ver
t
). Similar to Zhai
et al. (2024a), we treat the model with parameter Î¸ as our
policy network Ï€Î¸ : S â†’ Vn, and adopt PPO (Schulman
et al., 2017) as the backbone RL algorithm for updating Ï€Î¸.
Sequential revision. For modeling the state-action transition, we adopt the sequential revision formulation (Snell
et al., 2024). Specifically, at time step t = 0 the initial
input v
in
0
consists of the system prompt. For subsequent
time steps (t â‰¥ 1), the input prompt v
in
t
comprises the system prompt concatenated with all prior model and verifier
outputs, denoted by [v
out
k
, v
ver
k
]
tâˆ’1
k=0. An illustration of the
sequential revision is provided in Figure 2 (also see Figure
5 of Snell et al. (2024)), and an example of the state-action
transition is shown in Figure 3.
4. Evaluation Tasks
To evaluate the generalization of different post-training
methods, we select two tasks that each offer rule and visual variations. The first task, GeneralPoints, is a
new environment we have designed that allows assessment
of arithmetic reasoning abilities (Section 4.1). The second
task, V-IRL (Yang et al., 2024a), is chosen to examine the
modelâ€™s reasoning capabilities in an open-world visual navigation domain (Section 4.2).
4.1. The General Points Environment
Our original GeneralPoints environment, instantiated
on top of the Points24 environment (Zhai et al., 2024a),
is designed to evaluate generalization of arithmetic reasoning. Each state s of the environment contains 4 cards, described as text (in the GP-L variant) or presented as an image (in the GP-VL variant); see Figure 2 left for a visual
example of GeneralPoints. The goal is to produce an
equation that equals a target number (24 by default) using all 4 numbers from the cards exactly once. Detailed
examples of the state-action transitions are provided in Appendix A.2. Note that when input from GeneralPoints
is presented in an image (GP-VL), it naturally introduces
additional visual challenges requiring the VLM to recognize all cards before solving the equation.
3
SFT Memorizes, RL Generalizes
Q: Compute 24 using these four cards: [5, 4, 10, 7]
(V)LM 10+7+4+5 (7-4)*10-6 (7-5)*10+4
wrong
calculation


Reward: -1
illegal number
used


Reward: -5
correct answer



Reward: +10
Verifier Info:
Figure 2: An example of the sequential revision formulation with a verifier. The model generate the next answer v
out
t+1 conditioned
on all previous answers and information (v
out
i
, v
ver
t
, 0 â‰¤ i â‰¤ t) from the verifier.
System Prompt (v
in
0
)
[Task Description] You are an expert in {task name}, you are observing {purely language/vision-language
inputs + <image>}. You are currently at {state related info}. Please follow {tasks rules}.
[Output] Your response should be a valid json file in the following format:
{task related information and answer}
Appending previous model and verifier outputs to obtain v
in
t
v
in
t = [v
out
0
, v
ver
0
, v
out
1
, v
ver
1
, . . . , v
out
tâˆ’1
, v
ver
tâˆ’1
] â–· v
in
t = concat
v
in
0
, [v
out
k
, v
ver
k
]
tâˆ’1
k=0
Model output (v
out
t
) and Verifier Output (v
ver
t
)
{Task related json outputs}, {You success/fail}. â–· v
in
t+1 = concat(v
in
t
, v
out
t
, v
ver
t
)
Figure 3: An template of our prompt update for constructing v
in
t+1. The brown parts marks the task and related information, and
the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.
Rule variations. To study whether the model
learns arithmetic operations or simply memorizes the
post-training data, we introduce rule variations in
GeneralPoints. These variations consist of interpreting the symbols 'J', 'Q', and 'K' either as '11', '12', and
'13', respectively, or all as the same number '10'. These
variations ensure a rigorous evaluation of the modelâ€™s
ability to generalize arithmetic reasoning across diverse
settings. Each rule is specified as text in the input prompt,
see the {tasks rules} part in Figure 3. For studying ruled
based generalization, we post-train the model using one
rule, then evaluate using a different rule.
Visual variations. The GeneralPoints environment
can also be naturally customized to evaluate generalization
across visual variants. Since the major visual challenge is
to recognize the number of each card, agnostic to the the
color of the cards, we consider the cards with different colors as visual variants of the task. In the visual generalization setting, we train the model using cards of one color,
then test OOD performance using the other color.
4.2. The V-IRL Environment
While the GeneralPoints environment is designed to
assess arithmetic reasoning abilities, we further utilize the
V-IRL environment (Yang et al., 2024a) to study spatial
reasoning ability in an open-world navigation domain that
uses realistic visual input. As in GeneralPoints we
consider two versions of the environment, one (V-IRL-L)
that consists of pure language descriptions,2
and another
(V-IRL-VL) that includes vision-language input. The major visual challenge in V-IRL involves recognizing different landmarks from the visual observation3 before taking
an action. The goal is to navigate to a target location by
following a set of instructions that contain spatial information. A detailed example of one environment step is shown
in Appendix B.2.
Rule variations. To evaluate whether the model possesses spatial knowledge or simply memorizes posttraining data, we consider two distinct action space configurations. The first variant utilizes an absolute orientation action space, which includes {'north', 'northeast', 'east',
'southeast', 'south', 'southwest', 'west', 'northwest'}. The
second variant employs a relative orientation action space,
containing {'left', 'right', 'slightly left', 'slightly right'}. This
relative configuration adjusts the current orientation by 90
degrees or 45 degrees to the left or right, respectively. An
2The visual input can be parsed into pure text description, see
more details in Yang et al. (2024a) and an illustration of pure text
the version in Figure 14.
3
See Figure 4, the model needs to recognize landmarks like
The Dutch, Lola Taverna, and Shuka from the visual observation, and relate these landmarks with the textual instructions for
taking the right action.
4
SFT Memorizes, RL Generalizes
Shuka
Mediterranean
â­ First, turn slightly right towards the northeast and walk a short distance until you reach the next intersection,
where youâ€˜ll see The Dutch on your right. Next, make a sharp left turn to head northwest. Continue for a while
until you reach the next intersection, where Lola Taverna will be on your right. Finally, turn slightly right to face
northeast and walk a short distance until you reach your destination, Shuka, which will be on your right.
The Dutch
American restaurant
Lola Taverna
Greek
[OBSERVATION]
â€œStart!â€
[ACTION]
â€œTurn to northeast.â€
[OBSERVATION]
â€œSee Lola Taverna on my right.â€
[ACTION]
â€œLeft turn to northwest.â€
[OBSERVATION]
â€œSee Shuka on my right.â€
[ACTION]
â€œStop.â€
[OBSERVATION]
â€œSee The Dutch on my right.â€
[ACTION]
â€œLeft turn to northwest.â€
Figure 4: Demonstration of one navigation task in V-IRL. Agent navigates from place to place following the given linguistic
navigation instructions in V-IRL. The navigation procedure is shown at the top, with the navigation instructions displayed below.
Visual observation-related information is highlighted in green, while action-related information is marked in orange.
overview of a navigation task in V-IRL is provided in Figure 4, and a detailed state-action transition in V-IRL is
provided in Figure 13 (in Appendix B.2).
Visual variations. The key visual challenge in V-IRL is
to recognize landmarks from the visual observations (e.g.,
the green parts in Figure 4). Since the V-IRL environment contains visual observations from different cities, we
can assess visual generalization in V-IRL by training the
model to navigate in one location and then evaluate its performance in different locations.
5. Results
In this section, we present experiments that investigate
the generalization abilities induced by post-training with
RL and SFT. We adopt Llama-3.2-Vision-11B (Dubey
et al., 2024) as the backbone model. Following the
standard pipelines of RLHF (Ouyang et al., 2022) and
RL4VLM (Zhai et al., 2024a), we initialize the model with
SFT before running RL. We specifically study the following questions. Section 5.1: how does SFT or RL affect
the modelâ€™s generalization to different rules? Section 5.2:
when the model contains a visual component, how does
RL/SFT affect its generalization to different visual variants? Section 5.3: how does RL/SFT affect visual recognition capability in a VLM? Section 5.4: what role does SFT
play in RL training? Section 5.5: how does the number of
verification iterations affect generalization?
5.1. Generalization across Rules
We evaluate the performance of different post-training
methods on GeneralPoints and V-IRL, each of which
has a pure language (-L) and a vision-language (-VL) variant, and each encompassing rule variations. For each task,
we separately scale the training compute for RL and SFT
on a single rule. We consider the results on the trained rule
as in-distribution (ID) performance, whereas results on the
unseen rules measures out-of-distribution (OOD) generalization. In GeneralPoints, the ID case treats all 'J', 'Q',
'K' as 10, and the OOD cases interprets them as 11, 12, and
13. As for V-IRL, the ID case adopts the absolute orienta5
SFT Memorizes, RL Generalizes
0.0 0.5 1.0
1e10
28
52
76
100 GP-L
0 2 4
1e9
70
80
90
100
V-IRL-L
2 4 6
1e10
14
28
41
55
GP-VL
2 4
1e10
69
77
85
93
V-IRL-VL
0.0 0.5 1.0
1e10
1
6
12
17
0 2 4
1e9
0
33
65
98
2 4 6
1e10
3
8
14
19
2 4
1e10
0
19
37
56
In-distribution Out-of-distribution
Training Computation (GFLOPs)
pure language Initialization SFT RL with vision
Figure 5: Success rate (%) - GFLOPs trendlines for RL and SFT on GeneralPoints and V-IRL. The top row
shows in-distribution performance, while the bottom row shows out-of-distribution performance. Results are presented
for both pure language (-L) and vision-language (-VL) variants of each task. For GeneralPoints, we report the
episode success rate, while for V-IRL, we report per-step accuracy with overall success rate in Figures 1 and 18. Detailed
evaluation setups (and curve smoothing) are provided in Appendix C.3.
tion coordinate system and the OOD case uses the relative
orientation action space. Other details and additional experimental setup can be found in Appendix C.
RL generalizes, SFT memorizes. As illustrated in Figure 5, RL consistently improves OOD performance on
all tasks, including both unimodal (LLM) and multimodal (VLM). Specifically, Figure 6 demonstrates that
RL achieves an increase of +3.5% on GP-L (11.5% â†’
15.0%) and +11.0% on V-IRL-L (80.8% â†’ 91.8%).
Even with the additional challenge of visual recognition in
the VLM, RL maintains consistent performance improvements of +3.0% (11.2% â†’ 14.2%) on GP-VL and +9.3%
(35.7% â†’ 45.0%) on V-IRL-VL, respectively. In contrast, SFT consistently exhibits performance degradation
across all OOD evaluations on all tasks: -8.1% on GP-L
(11.5% â†’ 3.4%), -79.5% on V-IRL-L (80.8% â†’ 1.3%),
-5.6% (11.2% â†’ 5.6%) on GP-VL, and -33.2% (35.7% â†’
2.5%) on V-IRL-VL.
5.2. Generalization in Visual Out-of-Distribution Tasks
Section 5.1 demonstrates that RL yields generalization
across rule variations, whereas SFT exhibits the opposite
trend. Since VLMs also incorporate a visual modality, we
next study the effects of visual variation in OOD generalization. For GeneralPoints, we train the VLM using
the black suits (â™ , â™£) and test out-of-distribution performance on the red suits (â™¥, â™¦). For V-IRL, we train the
model on routes collected in New York City and evaluate it on the original V-IRL VLN mini benchmark (Yang
et al., 2024a) containing routes from various cities worldwide (see Appendix B.1 for details). Note that the rules
remain consistent across experiments in this section.
RL generalizes in visual OOD tasks. As shown in Figure 7, we observe that RL still generalizes in visual OOD
tasks, while SFT continues to suffer. Specifically, in
GP-VL and VIRL-VL, RL achieves performance improvements of +17.6% (23.6% â†’ 41.2%), +61.1% (16.7% â†’
77.8%), whereas SFT suffers from performance decreases
of -9.9% (23.6% â†’ 13.7%) and -5.6% (16.7% â†’ 11.1%).
As a byproduct of this visual OOD study, we also show
that our multi-turn RL formulation improves the state-ofthe-art results (see Table 5 of Yang et al. (2024a)) on the
V-IRL mini benchmark by +33.8% (44.0% â†’ 77.8%).
Notably, unlike the previous state-of-the-art approach reported in V-IRL, which relies on a two stage VLM-LLM
collaboration technique and tailored prompt engineering
on closed-sourced model (OpenAI, 2023a), our end-to-end
RL approach enables an open-sourced model (Dubey et al.,
2024) to reach superior performance.
5.3. RL Improves Visual Capabilities
Building upon the above observation that VLMs trained
with RL generalize to visual OOD tasks (Section 5.2), we
6
SFT Memorizes, RL Generalizes
0 3 6 9 12 15
SFT
Init
RL
3.4%
11.5%
15.0%
GP-L
0 20 40 60 80 100
1.3%
80.8%
91.8%
VIRL-L
0 4 8 12 16 20
5.6%
11.2%
14.2%
GP-VL
0 10 20 30 40 50
2.5%
35.7%
45.0%
V-IRL-VL
OOD success rate (%)
Figure 6: Comparison of out-of-distribution performance under rule variants. We report the success rate for
GeneralPoints and per-step-accuracy for V-IRL. For each subplot, RL and SFT are trained with equal computation, and their shared initial checkpoint (marked as Init) is set as baseline. Detailed setups are provided in Appendix C.3.
2 4
GFLOPs 1e10
10
20
30
40
SFT Init RL
10
20
30
40
13.7%
23.6%
41.2%
2.5 5.0 7.5
GFLOPs 1e10
20
40
60
80
SFT Init RL
20
40
60
80
11.1%
16.7%
77.8%
V-OOD Success Rate (%)
GP-VL V-IRL-VL
Initialization SFT RL Previous SOTA
Figure 7: Comparison of out-of-distribution performance under visual variants. Similar to Figures 5 and 6, we present
both the performance dynamics (shown as lines) and final performance (shown as bars) for visual out-of-distribution
evaluations. The previous state-of-the-art on V-IRL VLN mini benchmark (Yang et al., 2024a) is marked in orange.
Detailed evaluation setups (and curve smoothing) are provided in Appendix C.3.
consider a natural follow-up question: How does RL affect VLMsâ€™ visual capabilities? To study this question, we
conducted additional ablation studies in the GP-VL environment to investigate the OOD performance of RL and
SFT, along with the modelâ€™s visual recognition accuracy,
in terms of recognizing the 4 cards from the input image.
In particular, we study how scaling post-training compute
via RL/SFT both affects generalization in rule-based OOD
(Figure 8 left), and visual recognition accuracy and visual
OOD (Figure 8 right).
Scaling RL improves visual recognition accuracy in
VLM training. As shown in Figure 8, we observe that
the VLMâ€™s visual recognition accuracy largely affects
the overall performance, which was similarly observed
in Zhong et al. (2024). In addition, scaling up RL compute
also improves visual recognition accuracy, as a byproduct
of its generalization capability, while scaling SFT deteriorates both visual recognition accuracy and overall performance. Additional experimental results are provided
in Figures 16 and 17 of Appendix D.1.
5.4. The Role of SFT for RL Training
Despite the superiority of RL in generalizing the modelâ€™s
reasoning and visual capabilities, as discussed previously,
the experimental pipeline still instantiates RL after SFT.
In this subsection, we focus on another key question: Is
SFT necessary for RL training? To answer this question,
we conduct additional experiments that directly apply endto-end RL to post-train the base model Llama3.2 using
GeneralPoints in the purely language case (Figure 9).
SFT is necessary for RL training when the backbone
model does not follow instructions. Figure 9 shows
that without SFT, all end-to-end RL runs fail to improve.
More specifically, we observe that without SFT, the base
model suffers from poor instruction following capability.
A detailed failure case is provided in Figure 20 (in Appendix D.3), revealing that the base Llama-3.2-Vision-11B
model tends to generate long, tangential, and unstructured
responses. This issue makes it impossible to retrieve taskrelated information and rewards for RL training. Note that
due to the difference in backbone model, our results do not
contradict with DeepSeekAI et al. (2025), which suggests
that SFT is unnecessary for downstream RL training.
7
SFT Memorizes, RL Generalizes
0 10 20 30 40 50 60
30
50
70
90
Rule Variants
0 10 20 30 40 50 60
30
50
70
90
Visual Variants
4e10
5e10
6e10
7e10
8e10
Computation (GFLOPs)
GP-VL Success Rate (%)
Recognition Accuracy (%)
Out-of-distribution
In-distribution
SFT RL Init
Figure 8: Recognition vs. success rate for RL and SFT under different variants in GP-VL. We report both indistribution (red) and OOD (blue) performance of recognition (y-axis) and episode success rate (x-axis). We denote the
training compute of each data point via transparency (color bar) while connected (â‹†-â—¦) pairs are evaluated using same
checkpoints. As scaling up post-training compute, RL improves both recognition and overall accuracy, while SFT shows
opposite effect.
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Computation (GFLOPs) 1e10
0.0
0.1
0.2
0.3
0.4
Success Rate (%)
1e-6
2e-6
5e-7
Figure 9: RL experiments on GP-L without SFT initialization. All trials fail due to poor instruction following
capability of the base model.
5.5. Role of Verification Iterations
Verification serves as another crucial component in our
multi-step training and evaluation pipeline (see Figures 2
and 3). To validate its necessity and better understand its
effect, we conduct RL experiments with different verification iterations {1, 3, 5, 10} using GP-L (Figure 10).
Scaling up verification improves generalization.
In Figure 10, we observe that RL generalizes better with
more verification steps. More specifically, under the same
computational budget across all experiments, we observe
improvements of +2.15% (3 steps), +2.99% (5 steps),
+5.99% (10 steps). In contrast, in the case with one
verification step, we only observe a marginal improvement
of +0.48% in OOD performance improvement.
6. Conclusion, Discussion, and Limitations
In this paper, we present a comprehensive analysis of the
generalization effects of foundation model post-training
techniques, specifically RL and SFT. Through extensive 0 1 2 3 4 5 6
Out-of-distribution Growth (%)
4
6
8
10
12
In-distribution Growth (%)
VIter 1 VIter 3 VIter 5 VIter 10
3e9
6e9Computation (GFLOPs)
Figure 10: In-distribution vs. OOD performance growth
on GP-L. We record RL experiments with different number of verification iterations (VIter) as scaling up training
compute (color transparency).
experiments on the GeneralPoints and V-IRL tasks,
we demonstrated that RL exhibits superior performance
in learning generalizable knowledge, while SFT tends to
merely memorize the training data, across both the rule
and visual variations. This phenomenon consistently occurs across multimodal arithmetic and spatial reasoning capabilities. In addition, we studied the effect of RL on visual recognition, the role of SFT, and the role of verification
steps. During our study, two challenges were not resolved.
Failure of SFT on GP-VL. In Figure 5 for GP-VL,
we observe that SFT fails to achieve a comparable indistribution performance with RL. To mitigate the variance introduced by hyperparameter choices, we additionally conduct 10 more experiments with different learning
rates and tunable components (Figure 16), none of which
exhibits a strong increasing trend like RL (Figure 17).
8
SFT Memorizes, RL Generalizes
Given our observation that scaling up SFT degrades visual
recognition capabilities (Figure 8), we hypothesize that
SFT locally overfits to reasoning tokens while neglecting
recognition tokens, possibly due to the higher frequency of
reasoning tokens (see Figure 11 as example). We leave further investigation to future work.
Limits of RL in corner cases. As discussed in Section 5.4, SFT is necessary for effective RL training on
Llama-3.2. We investigate applying RL to an overly-tuned
SFT checkpoint. As demonstrated in Figure 19, RL is unable to recover out-of-distribution performance when starting from such a checkpoint. Example failure cases are illustrated in Figure 21, where the model collapses to the
training rule. These results, together with findings in Section 5.4, indicate that RL has limited effectiveness when
applied to extremely underfit or overfit initial checkpoints.
Further research is needed to delineate the conditions under
which SFT facilitates effective RL.
Impact Statement
This paper presents work aimed at advancing the field of
Machine Learning. While the study includes tasks such as
GeneralPoints, which is a synthetic environment, and
V-IRL, a real-world map simulator, our work is confined
to controlled research settings. The V-IRL environment is
designed as a simulated proxy for real-world tasks, but no
deployment or interaction with actual real-world systems or
data was involved. The methods, environments, and tasks
investigated in this study were constructed to advance our
understanding of model generalization without introducing
any foreseeable societal or ethical implications.
Acknowledgements
YZ would like to thank Xiaoxuan Feng for beautifying Figure 4. We would like to thank Jincheng Mei and Doina Precup for feedbacks on earlier manuscripts. Yi Ma would like
to acknowledge support from the joint Simons FoundationNSF DMS grant #2031899, the ONR grant N00014-22-1-
2102, the NSF grant #2402951, and also support from and
the HKU startup, the Hong Kong Center for Construction
Robotics Limited (HKCRC) Award 052245, and JC Club
of Hong Kong.
References
Abdulhai, M., White, I., Snell, C., Sun, C., Hong, J., Zhai,
Y., Xu, K., and Levine, S. LMRL Gym: Benchmarks for
multi-turn reinforcement learning with language models.
arXiv preprint arXiv:2311.18232, 2023. 2
Agarwal, A., Jiang, N., Kakade, S. M., and Sun, W. Reinforcement learning: Theory and algorithms. CS Dept.,
UW Seattle, Seattle, WA, USA, Tech. Rep, 32, 2019. 3
Allen-Zhu, Z. ICML 2024 Tutorial: Physics of Language Models, July 2024. Project page: https://
physics.allen-zhu.com/. 2
Allen-Zhu, Z. and Li, Y. Physics of language models: Part
3.1, knowledge storage and extraction. arXiv preprint
arXiv:2309.14316, 2023a. 1, 2
Allen-Zhu, Z. and Li, Y. Physics of language models: Part 3.2, knowledge manipulation. arXiv preprint
arXiv:2309.14402, 2023b. 2
Allen-Zhu, Z. and Li, Y. Physics of language models: Part
3.3, knowledge capacity scaling laws. arXiv preprint
arXiv:2404.05405, 2024. 2
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al. On the opportunities and risks
of foundation models. arXiv preprint arXiv:2108.07258,
2021. 1
Bousquet, O. and Elisseeff, A. Algorithmic stability and
generalization performance. volume 13, 2000. 1
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot
learners. Advances in neural information processing systems, 33:1877â€“1901, 2020. 1, 2
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F.,
and Zhang, C. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646,
2022. 2
Chen, G., Liao, M., Li, C., and Fan, K. AlphaMath almost zero: Process supervision without process. arXiv
preprint arXiv:2405.03553, 2024a. 3
Chen, J., Han, X., Ma, Y., Zhou, X., and Xiang, L. Unlock the correlation between supervised fine-tuning and
reinforcement learning in training code large language
models. arXiv preprint arXiv:2406.10305, 2024b. 2
Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J.,
Zhao, F., and Lin, D. ShareGPT4V: Improving large
multi-modal models with better captions. arXiv preprint
arXiv:2311.12793, 2023. 3
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416, 2022. 2
9
SFT Memorizes, RL Generalizes
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., et al. Training verifiers to solve math word problems.
arXiv preprint arXiv:2110.14168, 2021. 2, 3
DeepSeekAI et al. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning,
2025. URL https://arxiv.org/abs/2501.
12948. 1, 3, 7
Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle,
A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan,
A., et al. The Llama 3 Herd of models. arXiv preprint
arXiv:2407.21783, 2024. 2, 5, 6
Feng, X., Wan, Z., Wen, M., McAleer, S. M., Wen, Y.,
Zhang, W., and Wang, J. AlphaZero-like tree-search can
guide large language model decoding and training. arXiv
preprint arXiv:2309.17179, 2023. 3
Google, D. Introducing Gemini: Our largest
and most capable AI model, 2023. URL
https://blog.google/technology/ai/
google-gemini-ai/. 1, 2
Han, J., Zhan, H., Hong, J., Fang, P., Li, H., Petersson,
L., and Reid, I. What images are more memorable to
machines? arXiv preprint arXiv:2211.07625, 2022. 2
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,
Welbl, J., Clark, A., et al. Training compute-optimal
large language models. NeurIPS, 2023. 2, 18
Hosseini, A., Yuan, X., Malkin, N., Courville, A., Sordoni,
A., and Agarwal, R. V-STar: Training verifiers for selftaught reasoners. In First Conference on Language Modeling, 2024. URL https://openreview.net/
forum?id=stmqBSW2dV. 3
Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky,
A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. OpenAI o1 system card. arXiv preprint
arXiv:2412.16720, 2024. 1, 2, 3
Jiang, M., Liu, K. Z., Zhong, M., Schaeffer, R., Ouyang,
S., Han, J., and Koyejo, S. Investigating data contamination for pre-training language models. arXiv preprint
arXiv:2401.06059, 2024. 2
Kang, K., Setlur, A., Ghosh, D., Steinhardt, J., Tomlin, C.,
Levine, S., and Kumar, A. What do learning dynamics
reveal about generalization in LLM reasoning? arXiv
preprint arXiv:2411.07681, 2024. 1, 2
Kar, O. F., Tonioni, A., Poklukar, P., Kulshrestha, A., Zamir, A., and Tombari, F. Brave: Broadening the visual encoding of vision-language models. In European
Conference on Computer Vision, pp. 113â€“132. Springer,
2025. 3
Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. arXiv preprint
arXiv:2310.03744, 2023. 3
Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S.,
and Lee, Y. J. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024. URL
https://llava-vl.github.io/blog/
2024-01-30-llava-next/. 3
Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H.,
Cheng, H., Chang, K.-W., Galley, M., and Gao, J. MathVista: Evaluating mathematical reasoning of foundation
models in visual contexts. ICLR, 2023. 3
OpenAI. GPT-4, 2023a. URL https://openai.com/
research/gpt-4. 6
OpenAI. GPT-4 technical report. arXiv, pp. 2303â€“08774,
2023b. 1, 2
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray,
A., et al. Training language models to follow instructions
with human feedback. In NeurIPS, 2022. 2, 5
Qi, Z., Luo, H., Huang, X., Zhao, Z., Jiang, Y., Fan, X.,
Lakkaraju, H., and Glass, J. Quantifying generalization
complexity for large language models. arXiv preprint
arXiv:2410.01769, 2024. 2
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
et al. Improving language understanding by generative
pre-training. 2018. 2
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,
J., et al. Learning transferable visual models from natural language supervision. In International conference on
machine learning, pp. 8748â€“8763. PMLR, 2021. 2
Rahmanzadehgervi, P., Bolton, L., Taesiri, M. R., and
Nguyen, A. T. Vision language models are blind. In
Proceedings of the Asian Conference on Computer Vision, pp. 18â€“34, 2024. 3
Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel,
J., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi, Y.
Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for
natural language policy optimization. In The Eleventh
International Conference on Learning Representations,
2023. URL https://openreview.net/forum?
id=8aHzds2uUyB. 2
10
SFT Memorizes, RL Generalizes
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Klimov, O. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017. 3, 18
Setlur, A., Nagpal, C., Fisch, A., Geng, X., Eisenstein, J.,
Agarwal, R., Agarwal, A., Berant, J., and Kumar, A. Rewarding progress: Scaling automated process verifiers
for LLM reasoning. arXiv preprint arXiv:2410.08146,
2024. 3
Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling LLM testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314,
2024. 2, 3, 18
Sun, Z., Shen, S., Cao, S., Liu, H., Li, C., Shen, Y., Gan,
C., Gui, L., Wang, Y.-X., Yang, Y., Keutzer, K., and
Darrell, T. Aligning large multimodal models with factually augmented RLHF. In Ku, L.-W., Martins, A.,
and Srikumar, V. (eds.), Findings of the Association
for Computational Linguistics: ACL 2024, pp. 13088â€“
13110, Bangkok, Thailand, August 2024. Association
for Computational Linguistics. doi: 10.18653/v1/2024.
findings-acl.775. URL https://aclanthology.
org/2024.findings-acl.775. 2
Sutton, R. S. and Barto, A. G. Reinforcement Learning: An
Introduction. MIT press, 2018. 3
Tian, Y., Peng, B., Song, L., Jin, L., Yu, D., Mi, H.,
and Yu, D. Toward self-improvement of LLMs via
imagination, searching, and criticizing. arXiv preprint
arXiv:2404.12253, 2024. 3
Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M.,
Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., et al.
Cambrian-1: A fully open, vision-centric exploration of
multimodal LLMs. In NeurIPS, 2024a. 3
Tong, S., Fan, D., Zhu, J., Xiong, Y., Chen, X., Sinha, K.,
Rabbat, M., LeCun, Y., Xie, S., and Liu, Z. Metamorph:
Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024b.
2
Tong, S., Jones, E., and Steinhardt, J. Mass-producing failures of multimodal systems with language models. In
NeurIPS, 2024c. 3
Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S.
Eyes wide shut? Exploring the visual shortcomings of
multimodal LLMs. In CVPR, 2024d. 3
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E.,
Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,
2023. 2
Wang, X., Antoniades, A., Elazar, Y., Amayuelas, A.,
Albalak, A., Zhang, K., and Wang, W. Y. Generalization vs memorization: Tracing language modelsâ€™
capabilities back to pretraining data. arXiv preprint
arXiv:2407.14985, 2024. 2
Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W.,
Lester, B., Du, N., Dai, A. M., and Le, Q. V.
Finetuned language models are zero-shot learners.
In International Conference on Learning Representations, 2022a. URL https://openreview.net/
forum?id=gEZrGCozdqR. 2
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought
prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems, 35:
24824â€“24837, 2022b. 2
Yang, J., Ding, R., Brown, E., Qi, X., and Xie, S. V-IRL:
Grounding virtual intelligence in real life. In European
conference on computer vision, 2024a. 1, 2, 3, 4, 6, 7,
13, 14, 15
Yang, J., Yang, S., Gupta, A. W., Han, R., Fei-Fei, L., and
Xie, S. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv
preprint arXiv:2412.14171, 2024b. 3
Yang, Z., Lukasik, M., Nagarajan, V., Li, Z., Rawat, A. S.,
Zaheer, M., Menon, A. K., and Kumar, S. ResMem:
Learn what you can and memorize the rest. In Thirtyseventh Conference on Neural Information Processing
Systems, 2023. URL https://openreview.net/
forum?id=HFQFAyNucq. 2
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao,
Y., and Narasimhan, K. Tree of thoughts: Deliberate
problem solving with large language models. Advances
in Neural Information Processing Systems, 36, 2024. 1,
2
Ye, T., Xu, Z., Li, Y., and Allen-Zhu, Z. Physics
of language models: Part 2.1, grade-school math
and the hidden reasoning process. arXiv preprint
arXiv:2407.20311, 2024. 1, 2
Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G.,
Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. MMMU: A
massive multi-discipline multimodal understanding and
reasoning benchmark for expert AGI. In CVPR, 2024a.
3
Yue, X., Zheng, T., Ni, Y., Wang, Y., Zhang, K., Tong, S.,
Sun, Y., Yin, M., Yu, B., Zhang, G., et al. MMMUPro: A more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813,
2024b. 3
11
SFT Memorizes, RL Generalizes
Zelikman, E., Wu, Y., Mu, J., and Goodman, N. STaR:
Bootstrapping reasoning with reasoning. Advances
in Neural Information Processing Systems, 35:15476â€“
15488, 2022. 3
Zhai, Y., Bai, H., Lin, Z., Pan, J., Tong, S., Zhou, Y., Suhr,
A., Xie, S., LeCun, Y., Ma, Y., and Levine, S. Finetuning large vision-language models as decision-making
agents via reinforcement learning. In The Thirty-eighth
Annual Conference on Neural Information Processing Systems, 2024a. URL https://openreview.
net/forum?id=nBjmMF2IZU. 1, 2, 3, 5, 18
Zhai, Y., Tong, S., Li, X., Cai, M., Qu, Q., Lee, Y. J., and
Ma, Y. Investigating the catastrophic forgetting in multimodal large language model fine-tuning. In Conference on Parsimony and Learning, pp. 202â€“227. PMLR,
2024b. 3
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals,
O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):
107â€“115, 2021. 1
Zhang, C., Ippolito, D., Lee, K., Jagielski, M., TramÃ¨r, F.,
and Carlini, N. Counterfactual memorization in neural
language models. Advances in Neural Information Processing Systems, 36:39321â€“39362, 2023. 2
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,
et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 2
Zhong, M., Zhang, A., Wang, X., Hou, R., Xiong, W., Zhu,
C., Chen, Z., Tan, L., Bi, C., Lewis, M., et al. Law of
the weakest link: Cross capabilities of large language
models. arXiv preprint arXiv:2409.19951, 2024. 7
Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,
Efrat, A., Yu, P., Yu, L., et al. LIMA: Less is more for
alignment. Advances in Neural Information Processing
Systems, 36, 2024a. 2
Zhou, Y., Zanette, A., Pan, J., Levine, S., and Kumar, A.
ArCHer: Training language model agents via hierarchical multi-turn RL. arXiv preprint arXiv:2402.19446,
2024b. 2
Zhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans,
D., and Dai, H. Large language models can learn rules.
arXiv preprint arXiv:2310.07064, 2023. 1
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Finetuning language models from human preferences. arXiv
preprint arXiv:1909.08593, 2019. 2
12
SFT Memorizes, RL Generalizes
A. Details on the General Points Environment
In this section, we demonstrate the design details for
GeneralPoints mentioned in Section 4.1. We first
present the data used for this environment (Appendix A.1).
Then, we show examples of the environmentâ€™s transition
dynamics (Appendix A.2), followed by a description of key
arguments and reward design specification (Appendix A.3).
A.1. Data
GeneralPoints card quadruples are sampled from a
deck of 52 standard poker cards. Each sampled quadruple
is guaranteed to have at least one solution equals the target
point, i.e. 24. We ensure this by using an expert solver
during the sampling process.
A.2. Detailed Examples on the Transition Dynamics
As shown in Figure 11 and Figure 12, we treat the system
prompt as v
in
0
and then subsequently appending the future
outputs v
out
1:t
and verifier info v
ver
1:t
into the prompt for getting the t + 1 output. Figure 11 provides an example with
the visual inputs, while Figure 12 shows the language only
case.
A.3. Additional Eetails on the Environmental Design
Arguments. The GeneralPoints environment supports the following configurable arguments:
â€¢ Target point: Any positive integer
â€¢ Face cards rule: Two options
â€“ 'J', 'Q', and 'K' all count as '10'
â€“ 'J', 'Q', and 'K' count as '11', '12', and '13' respectively
â€¢ Card sampling: Two options
â€“ Sample 4 cards without replacement from a deck
of 52 poker cards
â€“ Sample at least one card from 'J', 'Q', and 'K'
â€¢ Card color: Three options
â€“ Black suits only: â™£, â™ .
â€“ Red suits only: â™¥, â™¦.
â€“ All suits: â™ , â™¥, â™£, â™¦.
For all experiments, we fix the target point at 24. In Figure 5, training and in-domain evaluation use the rule where
face cards count as '10'. For out-of-domain evaluation, we
use the alternative face cards rule and require at least one
face card, forcing calculations with numbers above 10 that
are not encountered during training. For visual distribution
shift experiments (Section 5.2), we train the model on black
suits â™ , â™£ and evaluate out-of-domain performance on red
suits â™¥, â™¦.
Reward design. An episode terminates when either a
correct equation is generated or the maximum verification
step of 5 is reached. The reward function is as follows:
â€¢ r = 5: For generating a legal equation that equals the
target point
â€¢ r = âˆ’1: For legal equations using each card once but
not equaling the target point
â€¢ r = âˆ’1: For exceeding maximum verification step
â€¢ r = âˆ’2: For legal equations containing numbers not
among the given choices
â€¢ r = âˆ’3: For all other illegal equations
In the vision-language variant (GeneralPoints-VL),
an additional penalty of r = âˆ’1.5 is applied when the agent
fails to correctly recognize the given cards.
B. Details on the V-IRL Environment
Similar to Appendix A, we present the design details for
V-IRL discussed in Section 4.2. First, we introduce
the database used for this environment (Appendix B.1)
and demonstrate transition examples (Appendix B.2). We
then describe the environment by explaining its fundamental componentâ€”route. Finally, we outline our modifications and reward design choices made to adapt the
original V-IRL for reinforcement learning training (Appendix B.3).
B.1. Data
Leveraging the data collection pipeline of Yang et al.
(2024a), we construct a training database with 1000 unique
routes from New York City. We evaluate all rule-variant
experiments and visual in-distribution experiments using
randomly sampled routes from this database. For visual
out-of-distribution experiments, we directly adopt the VLN
mini benchmark from Yang et al. (2024a). This benchmark
consists of 18 distinct routes across nine cities: Milan, New
Delhi, Buenos Aires, London, Hong Kong, New York,4
Melbourne, Lagos, and San Francisco, with two routes per
city.
4These NYC routes in the VLN mini benchmark do not overlap with our training data.
13
SFT Memorizes, RL Generalizes
System Prompt (v
in
0 )
[Task Description]
You are an expert 24 points card game player. You are observing these four cards
in the image. Note that 'J', 'Q', and 'K' count as '10', and each card must be used
once. Your goal is to output a formula that evaluates to 24 using numbers from the
cards and operators such as '+', '-', '*', '/', '(', ')', and '='.
[Output]
Your response should be a valid json file in the following format:
{
"cards": [x, y, z, w], where 'J', 'Q', and 'K' count as '10',
"number": [a, b, c, d], where a, b, c, and d are the numbers on the cards,
"formula": "an equation that equals 24",
}
Appending model and verifier outputs to obtain v
in
t
v
in
t = [v
out
0 , v
ver
0 , v
out
1 , v
ver
1 , . . . , v
out
tâˆ’1, v
ver
tâˆ’1] â–· v
in
t = concat
v
in
0 , [v
out
k , v
ver
k ]
tâˆ’1
k=0
Model output (v
out
t
)
{
"cards": ['A', '3', 'K', '6'],
"number": [1, 3, 13, 6],
"formula": "(1+6)*3+13=24",
}
Verifier Output (v
ver
t
)
You failed this trial because your formula is incorrect. â–· v
in
t+1 = concat(v
in
t
, v
out
t
, v
ver
t )
Figure 11: An example of our prompt update for constructing v
in
t+1 using v
in
t
, v
out
t and v
ver
t
. This example provides an optional vision
input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts
denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.
B.2. Detailed Examples on the Transition Dynamics
We provide detailed transition examples of the V-IRL environment in Figure 13 (vision and language) and Figure 14
(pure language).
B.3. Additional Details on the Environmental Design
Concept of route. The route serves as the fundamental
navigation object in the V-IRL environment. As illustrated in Figure 4, each route corresponds to a real-world
path with associated language instructions and visual signals. Using Figure 4 as an example, a route comprises:
â€¢ Destination: Shuka
â€¢ Starting point: Start
â€¢ Turning points: The Dutch, Lola Taverna
â€¢ Straight road: Roads connecting turning points, starting point, and destination
â€¢ Street views: 360-degree panoramic views at each
movable point
â€¢ Oracle information: Expert observation data for each
movable point
â€¢ Expert trajectory
â€¢ Instruction
Although the instructions in Figures 4, 13 and 14 are presented in different formats, they convey equivalent information, with Figure 4 using natural language.
Simplification and arguments. We simplify the original
V-IRL design from Yang et al. (2024a) to better accommodate RL training. The modifications include eliminating
the 2-stage navigation pipeline that required a separate visual detector for street view processing, and removing online queries to reduce training time and cost. Our V-IRL
environment contains 2 additional configuration arguments
compared with the original design:
â€¢ Action space: two options
â€“ Absolute direction:
"turn_direction(x)" where xâˆˆ{'north', 'northeast',
14
SFT Memorizes, RL Generalizes
System Prompt (v
in
0 )
[Task Description]
You are an expert 24 points card game player. You are observing these four cards in the image. Note that 'J', 'Q',
and 'K' count as '11', '12', and '13' respectively, and each card must be used once. Your goal is to output a formula
that evaluates to 24 using numbers from the cards and operators such as '+', '-', '*', '/', '(', ')', and '='.
[Input]
Cards: ['A', '3', 'K', '6']
[Output]
Your response should be a valid json file in the following format:
{
"cards": [x, y, z, w], where 'J', 'Q', and 'K' count as '10',
"number": [a, b, c, d], where a, b, c, and d are the numbers on the cards,
"formula": "an equation that equals 24",
}
Appending model and verifier outputs to obtain v
in
t
v
in
t = [v
out
0 , v
ver
0 , v
out
1 , v
ver
1 , . . . , v
out
tâˆ’1, v
ver
tâˆ’1] â–· v
in
t = concat
v
in
0 , [v
out
k , v
ver
k ]
tâˆ’1
k=0
Model output (v
out
t
)
{
"cards": ['A', '3', 'K', '6'],
"number": [1, 3, 13, 6],
"formula": "(1+6)*3+13=24",
}
Verifier Output (v
ver
t
)
You failed this trial because your formula is incorrect. â–· v
in
t+1 = concat(v
in
t
, v
out
t
, v
ver
t )
Figure 12: An example of our prompt update for constructing v
in
t+1 using v
in
t
, v
out
t and v
ver
t
. This example provides an optional vision
input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts
denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.
'east', 'southeast', 'south', 'southwest', 'west',
'northwest'}, "forward()", "stop()"
â€“ Relative direction:
"turn_direction(x)" where xâˆˆ{'left', 'right',
'slightly left', 'slightly right'}, "forward()",
"stop()"
â€¢ Maximum straight road length: any positive integer
The action space argument accommodates the rule variants
described in Section 4. For experiments shown in Figure 5, we use absolute direction action space during training and in-domain evaluation, while using the alternative
rule for out-of-domain evaluation. We implement a maximum straight road length to limit the number of movable
coordinates between turning points, preventing sequences
of repetitive "forward()" actions. We conduct visual distribution shift experiments (Section 5.2) via training the
model on New York City regions and evaluating the out-ofdomain performance on the worldwide navigation routes
from the benchmark released by Yang et al. (2024a).
Reward design. An episode terminates when either the
navigation agent stops at the destination or the maximum
verification step of 2 is reached. The reward function is as
follows:
â€¢ r = 1: For generating a correct action at the current
coordinate
â€¢ r = âˆ’1: For generating wrong action at the current
coordinate
â€¢ r = âˆ’1: For exceeding maximum verification step
â€¢ r = âˆ’1.5: For failed detection of landmarks
15
SFT Memorizes, RL Generalizes
System Prompt (v
in
0 )
[Task Description]
You are an expert in navigation. You will receive a sequence of instructions to
follow while observing your surrounding street views. You are also provided with
your observation and action history in text. your goal is to take the action based on
the current observation and instruction.
[Instruction]
1. First, turn left to face east.
2. Move forward until you reach the next intersection where Hotel 32One is on
your right behind.
3. Turn right to face north.
4. Move forward until you reach the next intersection where Dragon Gate Chinatown SF is on your right front.
5. Turn left to face east.
6. Move forward until the destination CafÃ© de la Presse is on your right.
[Current observation]
You observe a 2x2 grid of street view images with the following headings:
[front, right
back, left]
You need to identify if any of the landmarks in the instruction are visible in the street view grid.
[Action space]
- "forward()": indicates moving forward for 1 step;
- "turn_direction(x)": indicates turn direction to the target heading, where xâˆˆ[â€™northâ€™, â€™northeastâ€™, â€™eastâ€™, â€™southeastâ€™,
â€™southâ€™, â€™southwestâ€™, â€™westâ€™, â€™northwestâ€™];
- "stop()": indicates the navigation is finished;
[Observations and actions sequence]
O_1: No landmarks nearby;
A_1: turn_direction(east)
O_2: No landmarks nearby;
A_2: forward()
O_3: No landmarks nearby;
A_3: forward()
O_4: You observe an image of 4 views; You observe an intersection
A_4:
[Output]
Your response should be a valid json file in the following format:
{
"current observation": latest observation from the street view grid,
"current instruction": analyze the full instruction and identify the sentence to be executed,
"action": the action to be taken chosen from the action space,
}
Appending model and verifier outputs to obtain v
in
t
v
in
t = [v
out
0 , v
ver
0 , v
out
1 , v
ver
1 , . . . , v
out
tâˆ’1, v
ver
tâˆ’1] â–· v
in
t = concat
v
in
0 , [v
out
k , v
ver
k ]
tâˆ’1
k=0
Model output (v
out
t
)
{
"current observation": "Hotel 32One is on my right behind; I observe an intersection",
"current instruction": "Turn right to face north.",
"action": "turn_direction(north)",
}
Verifier Output (v
ver
t
)
Correct solution. â–· Move to the next scene upon correct action
Figure 13: An example of our prompt update for constructing v
in
t+1 using v
in
t
, v
out
t and v
ver
t
. This example provides an optional vision
input for VLMs, adding a visual recognition challenge. The brown parts marks the task and related information, and the purple parts
denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.
16
SFT Memorizes, RL Generalizes
System Prompt (v
in
0 )
[Task Description]
You are an expert in navgation. You will receive a sequence of instructions to follow. You are also provided with
your observation and action histroy in text. Your goal is to first analyze the instruction and identify the next sentence to be executed. Then, you need to provide the action to be taken based on the current observation and instruction.
[Instruction]
1. First, turn left to face east.
2. Move forward until you reach the next intersection where Hotel 32One is on your right behind.
3. Turn right to face north.
4. Move forward until you reach the next intersection where Dragon Gate Chinatown SF is on your right front.
5. Turn left to face east.
6. Move forward until the destination CafÃ© de la Presse is on your right.
[Action space]
- "forward()": indicates moving forward for 1 step;
- "turn_direction(x)": indicates turn direction to the target heading, where xâˆˆ['north', 'northeast', 'east', 'southeast',
'south', 'southwest', 'west', 'northwest'];
- "stop()": indicates the navigation is finished;
[Observations and actions sequence]
O_1: No landmarks nearby;
A_1: turn_direction(east)
O_2: No landmarks nearby;
A_2: forward()
O_3: No landmarks nearby;
A_3: forward()
O_4: Hotel 32One is on your right behind; You observe an intersection
A_4:
[Output]
Your response should be a valid json file in the following format:
{
"current observation": latest observation from the street view grid,
"current instruction": analyze the full instruction and identify the sentence to be executed,
"action": the action to be taken chosen from the action space,
}
Appending model and verifier outputs to obtain v
in
t
v
in
t = [v
out
0 , v
ver
0 , v
out
1 , v
ver
1 , . . . , v
out
tâˆ’1, v
ver
tâˆ’1] â–· v
in
t = concat
v
in
0 , [v
out
k , v
ver
k ]
tâˆ’1
k=0
Model output (v
out
t
)
{
"current observation": "Hotel 32One is on my right behind; I observe an intersection",
"current instruction": "Turn right to face north.",
"action": "turn_direction(north)",
}
Verifier Output (v
ver
t
)
Correct solution. â–· Move to the next scene upon correct action
Figure 14: An example of our prompt update for constructing v
in
t+1 using v
in
t
, v
out
t and v
ver
t
. The brown parts marks the task and
related information, and the purple parts denote the state (st) specific info. The brown parts marks the task and related information, and
the purple parts denote the state (st) specific info. The blue and red describe the output from the model and verifier, respectively.
17
SFT Memorizes, RL Generalizes
C. Experimental Setup
This section details the experimental setup used in Section 5. We first describe our data collection setup for supervised fine-tuning (Appendix C.1). Then, we present the
training pipeline (Appendix C.2). Finally, we describe our
evaluation metrics and the statistical tools used for generating plots (Appendix C.3).
C.1. Data
SFT data collection. As illustrated in Figures 11 to 14,
GeneralPoints and V-IRL environments naturally
align with prompt-response dialogue structures. We create training samples by pairing each system prompt with its
corresponding expert response. All SFT experiments in the
main body use optimal single-turn prompt-response pairs,
without any verification or revision steps.
SFT on sub-optimal trajectories To examine how more
diverse SFT data affects the out-of-distribution performance of SFT, we conduct an ablation study on GP-L using sub-optimal trajectories as training data. Unlike expert prompt-response pairs, these sub-optimal trajectories
include errors and verification messages in their prompts.
This format aligns with evaluation scenarios where multiple verification iterations are allowed, similar to the data
being used for the downstream RL training. In Figure 15,
we observe that SFT still merely memorizes the training
data with degraded out-of-distribution performance. This
evidence suggests that memorization occurs due to the fundamental nature of SFT training rather than the SFT data.
1 2 3 4 5
Computation (GFLOPs) 1e9
0
20
40
60
80
100
Success Rate (%)
Out-of-distribution
In-distribution
Figure 15: SFT experiments on GP-L with suboptimal
trajectories. Similar to results in Figure 5, SFT overfits
the training data even we increase the trajectory diversity.
C.2. Training Pipeline
As illustrated in Section 5, we follow the training pipeline
by RL4VLM (Zhai et al., 2024a), where we first initialize the model with SFT, then separately scale up the compute for SFT and RL (Schulman et al., 2017), starting from
this initialized model. For all experiments of SFT and RL
in the main body, we tune all components using a shared
learning rate per experiment. All training experiments are
conducted on an 8 H800 machine (80GB).
C.3. Evaluation Metric
Per-step accuracy. We report the per-step accuracy for
V-IRL-VL task in Figures 5 and 6. An individual step
is considered correct when the modelâ€™s chosen action
matches the expert trajectory at that position. Note that
intermediate verification steps are counted as independent
samples here.
Success rate. We report the success rate (%) of GP-L,
GP-VL, V-IRL-L and V-IRL-VL in Figures 5 and 6. In
the GeneralPoints task, success is defined as succeeding at least once during the inference time verification. In
the V-IRL task, a sample is recorded as success when the
model takes correct action at each movable point on the
route.
Computation estimation. We estimate the FLOPs for
training X following the similar manner of (Snell et al.,
2024; Hoffmann et al., 2023), where Xtrain = 6NDtrain
and Xinf erence = 2NDinf erence. Here, N represents the
model parameters and Dtrain represents the number of tokens during training. Suppose our SFT and RL experients
starts from a checkpoint trained on Dinit tokens, we can
estimate the training computation of SFT and RL via the
following equations:
XSF T = 6N(Dinit + DSF T )
XRL = 6N(Dinit + DRL) + 2NDbuf f er
Note that the used on-policy RL algorithm PPO (Schulman
et al., 2017) contains iterative stages of replay buffer collection and optimization, hence requiring additional inference
computation. For simplicity, we approximate the term via:
Dbuf f er â‰ˆ
E Â¯di
Â¯do
DRL
Â· DRL
= Î»DRL
where E âˆˆ N denotes the number of auto-regressive generation processes, Â¯di
,
Â¯do denote average input tokens and
output tokens. We estimate the Î» for GeneralPoints
and V-IRL as 6 and 5.1 respectively after calculation.
Line smoothing and error bar. All line plots in our paper adopt Savitzkyâ€“Golay filter with polynomial order 3 as
smoothing function. We assume each evaluated data point
18
SFT Memorizes, RL Generalizes
2 4 6
1e10
0
5
10
15
20
25
30
All Components Tunable
1e-7
5e-7
1e-6
1e-5
1e-4
2 4 6
1e10
0
5
10
15
20
25
Freeze Vision Encoder
1e-7
1e-6
2 4 6
1e10
0
5
10
15
20
25
30
Freeze Vision Encoder and Adapter
1e-7
5e-7
1e-6
Training Computation (GFLOPs)
Success Rate (%)
Figure 16: Ablation studies on GeneralPoints-VL SFT. We ablate the learning rate and report the in-distribution
episode success rate (%) of all experiments. None of the experiments shows an increasing trend beyond 30% success rate.
follows a binomial distribution and approximate the standard error using q
P (1âˆ’P )
N
, where P is the demical success
rate and N is the number of samples.
D. Additional Experimental Results
In this section, we provide additional experimental results
that are not covered in the main body.
D.1. Ablation Studies on GP-VL
As mentioned in Section 6, we observe an abnormal
phenomenon that SFT fails to achieve comparable indistribution performance with RL (see Figure 5 subplot row
1 column 3). To further explore this, we conduct ablation
studies over different hyperparameter choices.
SFT. We ablate the hyperparameter choices under the
same task setting of GP-VL in Section 5.1. For experiments fine-tuning all parameters, we search learning rates
from {1Ã—10âˆ’4
, 1Ã—10âˆ’4
, 1Ã—10âˆ’5
, 1Ã—10âˆ’6
, 5Ã—10âˆ’7
, 1Ã—
10âˆ’7}. Freezing the vision encoder, we search learning
rates {1 Ã— 10âˆ’6
, 1 Ã— 10âˆ’7}. Freezing vision encoder and
adapter, we search learning rates {1 Ã— 10âˆ’6
, 5 Ã— 10âˆ’7
, 1 Ã—
10âˆ’7}. We provide the in-distribution success rate curve
in Figure 16.
RL. Finding suitable hyperparameters for RL experiments requires minimal effort. We conduct a search over
learning rates 2 Ã— 10âˆ’6
, 1 Ã— 10âˆ’6
, with the in-distribution
success rate curves shown in Figure 17. All parameters are
tunable in our RL experiments.
D.2. More results on V-IRL-VL
Echoing per-step accuracy results in Figure 5, we report
the overall success rate of V-IRL-VL in Figure 18. Due to
the taskâ€™s complexity, both training methods achieve overall success rates no higher than 1%. For V-IRL, the overall
2.8 3.0 3.2 3.4 3.6
Training Computation (GFLOPs) 1e9
20
25
30
35
40
45
50
Success Rate (%)
1e-6
2e-6
Figure 17: Ablation studies on GeneralPoints-VL
RL. Echoing Figure 16, we ablate the learning rate and rreport the in-distribution episode success rate (%) of the two
experiments. All components are tunable here.
success rate is a significantly more demanding metric since
it aggregates per-step errors. For example, a random policy
achieving 10% per-step accuracy would achieve achieve
only approximately 10âˆ’8% success rate on enough routes
averaging 10 steps in length.
D.3. Failure Cases
In this section, we present 2 failure cases in our experiments as mentioned in Sections 5.4 and 6.
Without SFT, RL fails. In Figure 9, we present the training dynamics of failed RL experiments without SFT initialization. We additionally provide output examples of these
experiments in Figure 20, where the model tends to generate unstructured response and fail.
RL cannot save overfitted checkpoints. As shown
in Figure 19, RL cannot recover the out-of-distribution
performance when initialized from a extremely overfitted
checkpoint that has an initial per-step accuracy of less than
1%. We additionally provide an output example in Figure 19, where the model fails to adjust to the new rule.
19
SFT Memorizes, RL Generalizes
2 3 4 5
GFLOPs 1e10
10
20
30
40
50
60
V-IRL-VL
2 3 4 5
GFLOPs 1e10
0.00
0.25
0.50
0.75
1.00
1.25
1.50
V-IRL-VL OOD
Success Rate (%)
Initialization SFT RL
Figure 18: Overall success rate (%) - GFLOPs for
V-IRL-VL under rule variants. Due to the nature of
the task requiring aggregating a trajectory of correct actions, neither training method achieves reasonable outof-distribution performance.
0.5 1.0 1.5 2.0 2.5 3.0 3.5
Training Computation (GFLOPs) 1e9
0.0
0.5
1.0
1.5
Per-step Accuracy (%)
1e-7
2e-7
Figure 19: Out-of-distribution per-step accuracy
(%) - GFLOPs for V-IRL-VL under rule variants
with overfitted initial checkpoint. Evaluation metric
details can be found in Appendix C.3.
System Prompt (v
in
0 )
[Task Description]
You are an expert 24 points card game player. You are observing these four cards in the image. Note that 'J', 'Q',
and 'K' count as '11', '12', and '13' respectively, and each card must be used once. Your goal is to output a formula
that evaluates to 24 using numbers from the cards and operators such as '+', '-', '*', '/', '(', ')', and '='.
[Input]
Cards: ['2', 'J', '3', 'K']
[Output]
Your response should be a valid json file in the following format:
{
"cards": [x, y, z, w], where 'J', 'Q', and 'K' count as '10',
"number": [a, b, c, d], where a, b, c, and d are the numbers on the cards,
"formula": "an equation that equals 24",
}
Appending model and verifier outputs to obtain v
in
t
v
in
t = [v
out
0 , v
ver
0 , v
out
1 , v
ver
1 , . . . , v
out
tâˆ’1, v
ver
tâˆ’1] â–· v
in
t = concat
v
in
0 , [v
out
k , v
ver
k ]
tâˆ’1
k=0
Failed model output (v
out
t
)
To solve this problem, we can use a brute force approach by generating all possible combinations of the cards and
checking if any of them can be used to form an equation that equals 24.
Here is a Python solution for this problem:
â€œâ€˜python
def calculate_formula(cards):
# Define the mapping of cards to their corresponding numbers
. . .
Verifier Output (v
ver
t
)
You failed this trial because your formula is incorrect. â–· v
in
t+1 = concat(v
in
t
, v
out
t
, v
ver
t )
Figure 20: Example model outputs without SFT initialization. We record model responses using prompts similar to the
one shown in Figure 11. The results demonstrate that Llama-3.2-Vision-11B fails to follow instructions properly. We omit
the long response which tries to solve the puzzle via code but fails to finish within finite context length.
20
SFT Memorizes, RL Generalizes
System Prompt (v
in
0 )
[Task Description]
You are an expert in navigation. You will receive a sequence of instructions to follow while observing your surrounding street views. You are also provided with
your observation and action history in text. Your goal is to first analyze the instruction and identify the next sentence to be executed. Then, you need to provide the
action to be taken based on the current observation and instruction.
[Instruction]
1. First, turn right to face northwest.
2. Move forward until you reach next intersection where Korean War Memorial is
on your left.
3. Turn left to face southwest.
4. Move forward until you reach next intersection where Korean War Memorial is on your left behind.
5. Turn right to face north.
6. Move forward until you reach next intersection.
7. Turn left to face east.
8. Move forward until you reach next intersection.
9. Turn left to face north.
10. Move forward until you reach next intersection.
11. Turn right to face east.
12. Move forward until you reach next intersection.
13. Turn left to face northeast.
14. Move forward until you reach next intersection where 9/11 Memorial & Museum is on your left.
15. Turn right to face northwest.
16. Move forward until you reach destination where The destination 9/11 Memorial & Museum is on your right
front.
[Action space]
"forward()": indicates moving forward one step
"turn_direction(x)": indicates adjust the ego agent direction towards x direction. x could be any following [â€™leftâ€™,
â€™rightâ€™, â€™slightly leftâ€™, â€™slightly rightâ€™]
"stop()": indicates the navigation is finished.
[Observations and actions sequence]
O_1: You observe an image of 4 views;
A_1:
[Output]
{
"current observation": latest observation from the streetview grid,
"current instruction": analyze the full instruction and identify the sentence to be executed,
"action": the action to be taken chosen from the action space,
}
Appending model and verifier outputs to obtain v
in
t
v
in
t = [v
out
0 , v
ver
0 , v
out
1 , v
ver
1 , . . . , v
out
tâˆ’1, v
ver
tâˆ’1] â–· v
in
t = concat
v
in
0 , [v
out
k , v
ver
k ]
tâˆ’1
k=0
Failed model output (v
out
t
)
{
"current observation": "No landmarks nearby",
"current instruction": "Turn right to face northwest.",
"action": "turn_direction(northwest)",
}
Verifier Output (v
ver
t
)
Incorrect action. â–· Fail this sample.
Figure 21: Failed example of V-IRL transition due to overfitting. This phenomenon happens more frequently during
scaling up supervised fine-tuning.
21

---

### ğŸ¤– Cursor Assistant

I'll read the SFTvsRL paper carefully and revise experiments 7.4 and 7.5 with accurate details from the paper.

---

### ğŸ¤– Cursor Assistant

Let me search for more specific details from the paper:

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šä»”ç»†é˜…è¯»è¿™ç¯‡è®ºæ–‡ï¼Œå‡†ç¡®ä¿®æ”¹å®éªŒ 7.4 å’Œ 7.5 çš„å†…å®¹ã€‚è®©æˆ‘å…ˆç†è§£è®ºæ–‡çš„æ ¸å¿ƒå‘ç°ã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆäº†ï¼æˆ‘å·²ç»æ ¹æ®è®ºæ–‡ "SFT Memorizes, RL Generalizes" å‡†ç¡®ä¿®æ”¹äº†å®éªŒ 7.4 å’Œ 7.5ï¼š

## å®éªŒ 7.4ï¼ˆGeneralPointsï¼‰çš„å…³é”®ä¿®æ­£ï¼š

1. **ä»»åŠ¡æè¿°**ï¼šæ˜ç¡®äº†è¿™æ˜¯ç®— 24 ç‚¹çš„å¡ç‰Œæ¸¸æˆï¼Œæœ‰ GPâ€‘Lï¼ˆæ–‡æœ¬ï¼‰å’Œ GPâ€‘VLï¼ˆå›¾åƒï¼‰ä¸¤ä¸ªå˜ä½“
2. **è§„åˆ™å˜ä½“**ï¼šè®­ç»ƒæ—¶ J/Q/K=10ï¼Œæµ‹è¯•æ—¶åˆ†åˆ«ä¸º 11/12/13
3. **è§†è§‰å˜ä½“**ï¼šè®­ç»ƒç”¨é»‘è‰²èŠ±è‰²ï¼Œæµ‹è¯•ç”¨çº¢è‰²èŠ±è‰²
4. **å…·ä½“æ•°æ®**ï¼š
   - è§„åˆ™ OODï¼šRL åœ¨ GPâ€‘L æå‡ +3.5%ï¼ˆ11.5%â†’15.0%ï¼‰ï¼ŒSFT ä¸‹é™ â€‘8.1%ï¼ˆ11.5%â†’3.4%ï¼‰
   - è§†è§‰ OODï¼šRL æå‡ +17.6%ï¼ˆ23.6%â†’41.2%ï¼‰ï¼ŒSFT ä¸‹é™ â€‘9.9%ï¼ˆ23.6%â†’13.7%ï¼‰
5. **å…³é”®å‘ç°**ï¼šRL æ”¹å–„äº†åº•å±‚è§†è§‰è¯†åˆ«èƒ½åŠ›ï¼ŒSFT åˆ™å› è¿‡æ‹Ÿåˆæ¨ç† token è€Œå¿½è§†è§†è§‰ token
6. **éªŒè¯è¿­ä»£**ï¼š10 æ¬¡éªŒè¯å¸¦æ¥ +5.99% OOD æå‡ï¼Œ1 æ¬¡ä»… +0.48%

## å®éªŒ 7.5ï¼ˆVIRLâ€‘VLï¼‰çš„å…³é”®ä¿®æ­£ï¼š

1. **ç¯å¢ƒæè¿°**ï¼šä½¿ç”¨çœŸå®åœ°ç†æ•°æ®ä¸è¡—æ™¯å›¾åƒçš„å¼€æ”¾ä¸–ç•Œå¯¼èˆª
2. **è§„åˆ™å˜ä½“**ï¼šè®­ç»ƒç”¨ç»å¯¹æ–¹å‘ï¼ˆnorth/east ç­‰ï¼‰ï¼Œæµ‹è¯•ç”¨ç›¸å¯¹æ–¹å‘ï¼ˆleft/right ç­‰ï¼‰
3. **è§†è§‰å˜ä½“**ï¼šè®­ç»ƒç”¨çº½çº¦ 1000 æ¡è·¯çº¿ï¼Œæµ‹è¯•ç”¨å…¨çƒ 9 ä¸ªåŸå¸‚ 18 æ¡è·¯çº¿
4. **å…·ä½“æ•°æ®**ï¼š
   - è§„åˆ™ OODï¼šRL åœ¨ Vâ€‘IRLâ€‘L æå‡ +11.0%ï¼ˆ80.8%â†’91.8%ï¼‰ï¼ŒSFT æš´è·Œ â€‘79.5%ï¼ˆ80.8%â†’1.3%ï¼‰
   - è§†è§‰ OODï¼šRL æå‡ +61.1%ï¼ˆ16.7%â†’77.8%ï¼‰ï¼Œè¶…è¶Šå‰ SOTAï¼ˆ44.0%ï¼‰è¾¾ 33.8 ä¸ªç™¾åˆ†ç‚¹
5. **è¿‡ç¨‹å¥–åŠ±**ï¼šæ¯æ­¥ç»™äºˆ +1/â€‘1 åé¦ˆï¼Œåœ°æ ‡è¯†åˆ«é”™è¯¯é¢å¤– â€‘1.5 æƒ©ç½š
6. **éªŒè¯æœºåˆ¶**ï¼šverify_iter=2ï¼Œå…è®¸å•ç‚¹ä¸¤æ¬¡å°è¯•

è¿™äº›ä¿®æ”¹éƒ½åŸºäºè®ºæ–‡çš„å®é™…å®éªŒè®¾è®¡å’Œç»“æœï¼Œç¡®ä¿äº†å­¦æœ¯å‡†ç¡®æ€§ã€‚

---

### ğŸ‘¤ You

æ ¹æ® SimpleVLAâ€‘RL è®ºæ–‡ @https://arxiv.org/pdf/2509.09674 ç±»ä¼¼çš„ä¿®æ”¹å®éªŒ 7.6 æè¿°

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šæ ¹æ® SimpleVLA-RL è®ºæ–‡å‡†ç¡®ä¿®æ”¹å®éªŒ 7.6 çš„æè¿°ã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»æ ¹æ® SimpleVLA-RL è®ºæ–‡ï¼ˆhttps://arxiv.org/pdf/2509.09674ï¼‰å‡†ç¡®ä¿®æ”¹äº†å®éªŒ 7.6 çš„æè¿°ï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®å†…å®¹ï¼š

## ä¸»è¦ä¿®æ­£ç‚¹ï¼š

1. **èƒŒæ™¯ä¸æŒ‘æˆ˜**ï¼šæ˜ç¡®äº† VLA é¢ä¸´çš„ä¸¤å¤§æŒ‘æˆ˜ï¼ˆæ•°æ®ç¨€ç¼ºã€æ³›åŒ–ä¸è¶³ï¼‰ï¼Œä»¥åŠå— DeepSeek-R1 å¯å‘çš„åŠ¨æœº

2. **æ¡†æ¶è®¾è®¡**ï¼š
   - åŸºäº veRL æ„å»º
   - ä»…ä½¿ç”¨ç»“æœå¥–åŠ±ï¼ˆäºŒå…ƒæˆåŠŸ/å¤±è´¥ï¼‰
   - ä¸‰é¡¹æ¢ç´¢å¢å¼ºç­–ç•¥ï¼šåŠ¨æ€é‡‡æ ·ã€æ›´é«˜è£å‰ªç•Œ [0.8, 1.28]ã€æ›´é«˜æ¸©åº¦ 1.6
   - ç»„åˆæå‡çº¦ 30%

3. **å®éªŒç»“æœ**ï¼š
   - LIBERO è¾¾åˆ° 97.6% SOTA
   - å†·å¯åŠ¨å®éªŒï¼š1 æ¡è½¨è¿¹ SFT ä»… 17.3%ï¼ŒåŠ  RL è¾¾ 91.7%ï¼ˆ+74.4%ï¼Œç›¸å¯¹æå‡ 430%ï¼‰

4. **æ¨åˆ‡ç°è±¡**ï¼šè¯¦ç»†æè¿°äº† RL å‘ç°çš„æ–°åŠ¨ä½œæ¨¡å¼ï¼Œä»æœªåœ¨æ¼”ç¤ºä¸­å‡ºç°

5. **GRPO ç®—æ³•**ï¼šè§£é‡Šäº†å¦‚ä½•é€šè¿‡ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ä¼°è®¡ï¼Œæ— éœ€ä»·å€¼ç½‘ç»œå³å¯ä¼˜åŒ–

6. **åŠ¨ä½œåˆ†å—**ï¼š25 ä¸ªåŠ¨ä½œ/æ‰¹æ¬¡ï¼Œ50Hz æ‰§è¡Œï¼Œæ¨ç†é¢‘ç‡é™ä½ 25 å€

7. **ä»¿çœŸç¯å¢ƒ**ï¼šLIBERO + RoboTwin 2.0ï¼Œ8Ã—A800 GPUï¼Œ300 æ­¥çº¦ 4.3 å¤©

8. **æ³›åŒ–ç»´åº¦**ï¼šç©ºé—´ã€ç‰©ä½“ã€ç›®æ ‡ä¸‰ä¸ªç»´åº¦çš„æ³›åŒ–èƒ½åŠ›æå‡

9. **ä¸ VIRL-VL å¯¹æ¯”**ï¼šè¿‡ç¨‹å¥–åŠ± vs ç»“æœå¥–åŠ±çš„æƒè¡¡

æ‰€æœ‰å†…å®¹éƒ½åŸºäºè®ºæ–‡çš„å®é™…å®éªŒè®¾è®¡å’Œå‘ç°ï¼Œç¡®ä¿äº†å­¦æœ¯å‡†ç¡®æ€§ã€‚

---

### ğŸ‘¤ You

æ ¹æ®è®ºæ–‡ @https://arxiv.org/pdf/2505.13417 ä¿®æ”¹ 7.7 å†…å®¹

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šæ ¹æ® AdaptThink è®ºæ–‡å‡†ç¡®ä¿®æ”¹å®éªŒ 7.7 çš„æè¿°ã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»æ ¹æ® AdaptThink è®ºæ–‡ï¼ˆhttps://arxiv.org/pdf/2505.13417ï¼‰å‡†ç¡®ä¿®æ”¹äº†å®éªŒ 7.7 çš„æè¿°ï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®å†…å®¹ï¼š

## ä¸»è¦ä¿®æ­£ç‚¹ï¼š

1. **é—®é¢˜èƒŒæ™¯**ï¼šå¤§å‹æ¨ç†æ¨¡å‹çš„é•¿æ€è€ƒå¸¦æ¥æ¨ç†å¼€é”€ï¼Œç°æœ‰æ–¹æ³•ä»å¯¹æ‰€æœ‰é—®é¢˜åº”ç”¨æ€è€ƒ

2. **NoThinking å‘ç°**ï¼šé€šè¿‡ "<think></think>" æç¤ºè·³è¿‡æ€è€ƒï¼Œåœ¨ç®€å•é—®é¢˜ä¸Šæ€§èƒ½ç›¸å½“ç”šè‡³æ›´å¥½

3. **æ ¸å¿ƒé—®é¢˜**ï¼šèƒ½å¦å­¦ä¼šæ ¹æ®é—®é¢˜éš¾åº¦è‡ªé€‚åº”é€‰æ‹©æ€è€ƒæ¨¡å¼

4. **ç®—æ³•è®¾è®¡**ï¼š
   - çº¦æŸä¼˜åŒ–ç›®æ ‡ï¼šé¼“åŠ± NoThinking åŒæ—¶ç¡®ä¿æ€§èƒ½ä¸ä¸‹é™
   - é‡è¦æ€§é‡‡æ ·ï¼šå¹³è¡¡ä¸¤ç§æ¨¡å¼æ ·æœ¬ï¼Œå…‹æœå†·å¯åŠ¨

5. **å®éªŒç»“æœ**ï¼ˆDeepSeek-R1-Distill-Qwen-1.5Bï¼‰ï¼š
   - GSM8Kï¼šå“åº”é•¿åº¦ -50.9%ï¼Œå‡†ç¡®ç‡ +4.1%
   - MATH500ï¼šå“åº”é•¿åº¦ -63.5%ï¼Œå‡†ç¡®ç‡ +1.4%
   - AIME2024ï¼šå“åº”é•¿åº¦ -44.7%ï¼Œå‡†ç¡®ç‡ +1.6%

6. **è®­ç»ƒæœºåˆ¶**ï¼š
   - æ‹‰æ ¼æœ—æ—¥ä¹˜å­ Î´ æ§åˆ¶ NoThinking é¼“åŠ±å¼ºåº¦
   - Î´=0 ç­‰ä»·äºæ ‡å‡† RLï¼ŒÎ´ è¶Šå¤§è¶Šæ¿€è¿›
   - Î´ ä» 0 åˆ° 0.1 äº§ç”Ÿç­–ç•¥è°±ç³»

7. **è¡Œä¸ºæ¨¡å¼**ï¼šç®€å•é—®é¢˜ç”¨ NoThinkingï¼Œå›°éš¾é—®é¢˜ï¼ˆå¦‚ AIMEï¼‰ä¿ç•™å®Œæ•´æ€è€ƒé“¾

8. **ä¸è’¸é¦äº’è¡¥**ï¼šè’¸é¦é™ä½éœ€è¦æ€è€ƒçš„ä»»åŠ¡æ¯”ä¾‹ï¼ŒAdaptThink ä¼˜åŒ–æ€è€ƒè§¦å‘ç­–ç•¥

æ‰€æœ‰å†…å®¹éƒ½åŸºäºè®ºæ–‡çš„å®é™…æ–¹æ³•å’Œå®éªŒç»“æœï¼Œç¡®ä¿äº†å­¦æœ¯å‡†ç¡®æ€§ã€‚

---

### ğŸ‘¤ You

æ ¹æ®è®ºæ–‡ @https://arxiv.org/pdf/2504.11536 ä¿®æ”¹ 7.8 å†…å®¹ï¼Œéœ€è¦ä»æŠŠç¨‹åºè¯­è¨€ä½œä¸ºæ€è€ƒè¯­è¨€è§’åº¦ä»‹ç»ï¼Œä¿ç•™ "æˆ‘è¯­è¨€çš„è¾¹ç•Œå°±æ˜¯æˆ‘æ€è€ƒçš„è¾¹ç•Œ" çš„å“²å­¦æ€è€ƒï¼Œå¯ä»¥å‚è€ƒ chapter5.md å¼€å¤´çš„å†…å®¹ã€‚éœ€è¦å¼ºè°ƒï¼ŒReTool çš„ SFT é˜¶æ®µåœ¨å•æœº 8 å¡ä¸Šä»…éœ€ 1 å°æ—¶ï¼Œä½† RL é˜¶æ®µéœ€è¦ 9 å¤©ï¼Œä¹Ÿå°±æ˜¯ RL è¿œæ¯” SFT å¼€é”€é«˜ï¼Œè¿™æ˜¯å› ä¸º SFT å’Œ RL åœ¨æ ·æœ¬åˆ©ç”¨æ•ˆç‡ä¸Šçš„ä¸åŒã€‚

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šæ ¹æ® ReTool è®ºæ–‡å‡†ç¡®ä¿®æ”¹å®éªŒ 7.8 çš„æè¿°ï¼Œå¼ºè°ƒç¨‹åºè¯­è¨€ä½œä¸ºæ€è€ƒè¯­è¨€çš„å“²å­¦æ„ä¹‰ï¼Œä»¥åŠ SFT ä¸ RL çš„æˆæœ¬å·®å¼‚ã€‚è®©æˆ‘å…ˆæŸ¥çœ‹ chapter5.md çš„å¼€å¤´å†…å®¹ä½œä¸ºå‚è€ƒã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»æ ¹æ® ReTool è®ºæ–‡ï¼ˆhttps://arxiv.org/pdf/2504.11536ï¼‰å‡†ç¡®ä¿®æ”¹äº†å®éªŒ 7.8 çš„æè¿°ï¼Œä¸»è¦åŒ…å«ï¼š

## æ ¸å¿ƒä¿®æ­£ç‚¹ï¼š

1. **å“²å­¦æ¡†æ¶**ï¼šå¼•ç”¨ç»´ç‰¹æ ¹æ–¯å¦"æˆ‘è¯­è¨€çš„ç•Œé™æ„å‘³ç€æˆ‘ä¸–ç•Œçš„ç•Œé™"ï¼Œå¼ºè°ƒç¨‹åºè¯­è¨€æ‹“å±•æ€è€ƒè¾¹ç•Œ

2. **æ¡†æ¶è®¾è®¡**ï¼š
   - å·¥å…·å¢å¼ºçš„ RL æ¡†æ¶
   - å®æ—¶ä»£ç æ‰§è¡Œæ•´åˆåˆ°æ¨ç†å¾ªç¯
   - è‡ªä¸»æ¢ç´¢å·¥å…·ä½¿ç”¨ç­–ç•¥

3. **ä¸¤é˜¶æ®µè®­ç»ƒ**ï¼š
   - **SFT é˜¶æ®µ**ï¼šå†·å¯åŠ¨æ•°æ®æ„å»ºï¼ŒOpenThoughts â†’ ä»£ç é›†æˆè½¨è¿¹ï¼Œ**8 å¡ H200 ä»…éœ€ 1 å°æ—¶**
   - **RL é˜¶æ®µ**ï¼šäº¤ç»‡å®æ—¶ä»£ç æ‰§è¡Œçš„ rolloutï¼Œ**ç›¸åŒç¡¬ä»¶éœ€è¦ 9 å¤©å®Œæˆ 400 æ­¥**

4. **æˆæœ¬å·®å¼‚æ ¹æº**ï¼š
   - SFTï¼šæ¯ä¸ª token æä¾› $O(\text{token æ•°é‡})$ bits ä¿¡æ¯
   - RLï¼šæ¯ä¸ª episode ä»…æä¾› $O(1)$ bits ä¿¡æ¯
   - ReTool çš„ rollout åŒ…å«æ•°åƒ token + å¤šæ¬¡ä»£ç æ‰§è¡Œï¼Œä½†åªè·å¾—äºŒå…ƒä¿¡å·
   - å¯¼è‡´ RL æˆæœ¬æ¯” SFT é«˜ä¸¤ä¸ªæ•°é‡çº§

5. **å¥–åŠ±è®¾è®¡**ï¼šæç®€ç»“æœå¯¼å‘ï¼ˆ+1/-1ï¼‰ï¼Œä¸è€ƒè™‘ä»£ç å¯æ‰§è¡Œæ€§å¥–åŠ±

6. **å®éªŒç»“æœ**ï¼ˆAIMEï¼‰ï¼š
   - ReTool-32Bï¼š400 æ­¥è¾¾ 67.0%ï¼Œè¶…è¶Šæ–‡æœ¬ RLï¼ˆ1080 æ­¥ 40.0%ï¼‰
   - æ‰©å±•è®¾ç½®ï¼š72.5%ï¼Œè¶…è¶Š o1-preview 27.9 ä¸ªç™¾åˆ†ç‚¹
   - å†·å¯åŠ¨ï¼š40.9%ï¼ˆä»… SFTï¼‰ï¼Œè¶…è¶ŠåŸºç¡€æ¨¡å‹ 26.7%

7. **æ¶Œç°è¡Œä¸º**ï¼šä»£ç è‡ªæˆ‘ä¿®æ­£èƒ½åŠ›ï¼ˆ"aha moment"ï¼‰

8. **è¡Œä¸ºæ¼”åŒ–**ï¼š
   - å“åº”é•¿åº¦å‡å°‘ 40%ï¼ˆ10k â†’ 6k tokenï¼‰
   - ä»£ç æ¯”ç‡ã€è¡Œæ•°ã€æ­£ç¡®æ€§æŒç»­ä¸Šå‡
   - è°ƒç”¨æ—¶æœºæå‰ï¼Œç”¨é€”å¤šæ ·åŒ–

9. **æ·±å±‚æ„ä¹‰**ï¼šæ··åˆæ¨ç†èŒƒå¼ï¼Œæ–‡æœ¬è¯­è¨€ + ç¨‹åºè¯­è¨€äº’è¡¥

æ‰€æœ‰å†…å®¹éƒ½åŸºäºè®ºæ–‡çš„å®é™…æ–¹æ³•ã€å®éªŒç»“æœå’Œå‘ç°ï¼Œç‰¹åˆ«å¼ºè°ƒäº† SFT ä¸ RL çš„æˆæœ¬å·®å¼‚åŠå…¶æ ¹æºã€‚

---

### ğŸ‘¤ You

æ ¹æ® AWorldâ€‘train è®ºæ–‡ @https://arxiv.org/pdf/2508.20404 ä¿®æ”¹ 7.9 ä»‹ç»ï¼Œéœ€è¦å‚è€ƒé¡¹ç›® READMEï¼Œå¼ºè°ƒè®­ç»ƒä¸€ä¸ªèƒ½åˆ· GAIA æ¦œå•çš„ æ¨¡å‹æˆæœ¬å¾ˆé«˜ï¼Œè¿™ä¸ªå®éªŒä¸»è¦æ˜¯æ•™è‚²æ„ä¹‰ï¼Œç†Ÿæ‚‰ä¸€ä¸‹ rollout è¿‡ç¨‹

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šæ ¹æ® AWorld è®ºæ–‡å’Œé¡¹ç›® README å‡†ç¡®ä¿®æ”¹å®éªŒ 7.9 çš„æè¿°ï¼Œå¼ºè°ƒ GAIA çš„æŒ‘æˆ˜æ€§ã€è®­ç»ƒæˆæœ¬ä»¥åŠæœ¬å®éªŒçš„æ•™è‚²æ„ä¹‰ã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»æ ¹æ® AWorld è®ºæ–‡ï¼ˆhttps://arxiv.org/pdf/2508.20404ï¼‰å’Œé¡¹ç›® README å‡†ç¡®ä¿®æ”¹äº†å®éªŒ 7.9 çš„æè¿°ï¼Œä¸»è¦åŒ…å«ï¼š

## æ ¸å¿ƒä¿®æ­£ç‚¹ï¼š

1. **GAIA çš„æŒ‘æˆ˜æ€§**ï¼š
   - éªŒè¯é›†ä»… 165 é¢˜ï¼Œæµ‹è¯•é›†çº¦ 300 é¢˜ï¼ˆæ•°æ®ç¨€ç¼ºï¼‰
   - ä»»åŠ¡å¤æ‚ï¼Œå¹³å‡ 10-20 è½®äº¤äº’
   - Qwen3-32B-AWorld éœ€è¦ 2 å° 8Ã—A100 è®­ç»ƒå¤šå¤©ï¼Œä»…è¾¾ 32.23%
   - è·ç¦» SOTAï¼ˆ80%+ï¼‰è¿˜å¾ˆè¿œ

2. **æ•™è‚²å®šä½**ï¼š
   - é‡‡ç”¨æ•™è‚²å‹å¥½é…ç½®ï¼ˆå¦‚ Qwen3-4Bï¼‰
   - ç›®æ ‡æ˜¯æ¼”ç¤ºå®Œæ•´æµç¨‹ã€ç†è§£æœºåˆ¶ã€å®è·µ RL
   - é‡ç‚¹ä¸åœ¨åˆ·æ¦œ

3. **AWorld æ¡†æ¶**ï¼š
   - "ä»å®è·µä¸­å­¦ä¹ "èŒƒå¼çš„ä¸‰è¦ç´ ï¼šç®—æ³•ã€ç¯å¢ƒã€å…ˆéªŒ
   - æ ¸å¿ƒè´¡çŒ®ï¼š14.6 å€æ•°æ®æ”¶é›†åŠ é€Ÿ
   - è§£å†³ç»éªŒç”Ÿæˆæ•ˆç‡ç“¶é¢ˆ

4. **MCP æ²™ç›’ç”Ÿæ€**ï¼š
   - 26 ä¸ªæœåŠ¡å™¨ã€126 ä¸ªå·¥å…·å‡½æ•°
   - è¯¦ç»†åˆ—ä¸¾ï¼šWeb äº¤äº’ã€æ–‡æ¡£å¤„ç†ã€å¤šåª’ä½“ã€æ™ºèƒ½æ¨ç†ã€ä»£ç æ‰§è¡Œã€Excelã€çŸ¥è¯†æ£€ç´¢
   - é¿å…çœŸå® API çš„å°ç¦ä¸æ³¢åŠ¨

5. **åˆ†å¸ƒå¼æ¶æ„**ï¼š
   - é›†ç¾¤å¹¶å‘äº¤äº’
   - ä»å¤©çº§å‹ç¼©åˆ°å°æ—¶çº§
   - ç»Ÿä¸€åè®®ã€å¯é‡æ”¾ã€å¯å®¡è®¡

6. **è®­ç»ƒæµç¨‹**ï¼š
   - æ ‡å‡† RL èŒƒå¼
   - rollout æ˜¯æœ€è€—æ—¶ç¯èŠ‚
   - å¯¹æ¥ VeRLã€OpenRLHF ç­‰æ¡†æ¶

7. **å®éªŒç»“æœ**ï¼š
   - Qwen3-32B-AWorldï¼š32.23%
   - è¶…è¶Š GPT-4oï¼ˆ27.91%ï¼‰ï¼Œæ¥è¿‘ DeepSeek-V3ï¼ˆ31.89%ï¼‰
   - ä½†è·äººç±»æ°´å¹³ï¼ˆ80%+ï¼‰ä»æœ‰å·®è·ï¼Œæˆæœ¬æé«˜

8. **æ•™è‚²ä»·å€¼**ï¼š
   - å®Œæ•´ä½“éªŒ rollout æµç¨‹
   - ç†è§£ Agent æ„å»ºã€ç¯å¢ƒäº¤äº’ã€ç»éªŒæ”¶é›†ã€å¥–åŠ±è®¡ç®—ã€ç­–ç•¥æ›´æ–°
   - é‡ç‚¹æ˜¯ç†è§£æœºåˆ¶ï¼Œè€Œéåˆ·æ¦œ

9. **RL æ”¶ç›Š**ï¼šä»»åŠ¡åˆ†è§£ã€å·¥å…·ç¼–æ’ã€å¼‚å¸¸æ¢å¤ä¸‰æ–¹é¢èƒ½åŠ›æå‡

10. **å·¥ç¨‹ç»éªŒ**ï¼šæ²™ç›’ç¨³å®šæ€§ã€è´Ÿè½½å‡è¡¡ã€é•¿ä¸Šä¸‹æ–‡æ”¯æŒã€å¥–åŠ±è®¾è®¡

æ‰€æœ‰å†…å®¹éƒ½åŸºäºè®ºæ–‡å’Œ README çš„å®é™…æè¿°ï¼Œç‰¹åˆ«å¼ºè°ƒäº† GAIA çš„æŒ‘æˆ˜æ€§ã€è®­ç»ƒæˆæœ¬ä»¥åŠæœ¬å®éªŒçš„æ•™è‚²å®šä½ã€‚

---

### ğŸ‘¤ You

æ ¹æ® Intuitor è®ºæ–‡ @https://arxiv.org/pdf/2505.19590 ä¿®æ”¹ 7.10 ä»‹ç»ï¼Œå…¶ä¸­ç¬¬ä¸‰æ›²çº¿çš„è¯´æ³•å‚è€ƒé¡¹ç›® README

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šæ ¹æ® Intuitor é¡¹ç›® READMEï¼ˆå› ä¸º PDF è§£æå¤±è´¥ï¼‰å‡†ç¡®ä¿®æ”¹å®éªŒ 7.10 çš„æè¿°ï¼Œç‰¹åˆ«æ˜¯å…³äº"ç¬¬ä¸‰æ›²çº¿"çš„æ¦‚å¿µã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»æ ¹æ® Intuitor é¡¹ç›® README å‡†ç¡®ä¿®æ”¹äº†å®éªŒ 7.10 çš„æè¿°ï¼Œä¸»è¦åŒ…å«ï¼š

## æ ¸å¿ƒä¿®æ­£ç‚¹ï¼š

1. **ä¸‰æ¡æ›²çº¿æ¡†æ¶**ï¼š
   - **ç¬¬ä¸€æ›²çº¿ï¼ˆé¢„è®­ç»ƒï¼‰**ï¼šå­¦"æ˜¯ä»€ä¹ˆ"ï¼ˆçŸ¥è¯†ï¼‰ï¼Œä»£è¡¨ï¼šGPT-3/4ã€LLaMAã€Qwen
   - **ç¬¬äºŒæ›²çº¿ï¼ˆRLVRï¼‰**ï¼šå­¦"å¯¹ä¸å¯¹"ï¼ˆæ•°å­¦ã€ä»£ç æ­£ç¡®æ€§ï¼‰ï¼Œä»£è¡¨ï¼šDeepSeek-R1ã€o1ã€Kimi K1.5
   - **ç¬¬ä¸‰æ›²çº¿ï¼ˆæ— ç›‘ç£ RLï¼‰**ï¼šå­¦"å¥½ä¸å¥½"ï¼ˆé€šç”¨è´¨é‡æå‡ï¼‰ï¼Œä»£è¡¨ï¼šIntuitorã€TTRLã€Genius

2. **ç¬¬ä¸‰æ›²çº¿çš„æ–¹æ³•**ï¼š
   - å†…éƒ¨åé¦ˆï¼šIntuitorï¼ˆself-certaintyï¼‰ã€Absolute Zero
   - ä¸€è‡´æ€§ï¼šTTRLï¼ˆplurality votingï¼‰ã€Geniusï¼ˆself-consistencyï¼‰
   - è§„åˆ™å¥–åŠ±ã€æ–°é¢–æ€§ã€å¤šæ™ºèƒ½ä½“è¾©è®º

3. **Self-certainty å®šä¹‰**ï¼š
   - æ•°å­¦å…¬å¼ï¼šKL(U || p_Ï€)ï¼Œmode-seeking åº¦é‡
   - é«˜ç½®ä¿¡åº¦ = åˆ†å¸ƒå°–é”ï¼Œä½ç½®ä¿¡åº¦ = æ¥è¿‘å‡åŒ€
   - å¯¹é•¿åº¦åç½®ä¸æ•æ„Ÿ

4. **å®éªŒè®¾ç½®**ï¼š
   - Qwen2.5-3Bï¼ŒMATH æ•°æ®é›†
   - GRPO æ¡†æ¶ï¼Œæ¯é¢˜é‡‡æ · 7 ä¸ªå€™é€‰
   - ç”¨ self-certainty æ›¿ä»£å¤–éƒ¨å¥–åŠ±

5. **åŸŸå†…æ€§èƒ½**ï¼š
   - GSM8Kï¼š79.2% vs GRPO 82.6%ï¼ˆæ¥è¿‘ï¼‰
   - MATH500ï¼š61.2% vs GRPO 63.6%ï¼ˆæ¥è¿‘ï¼‰

6. **åŸŸå¤–æ³›åŒ–**ï¼ˆå…³é”®ä¼˜åŠ¿ï¼‰ï¼š
   - LiveCodeBenchï¼š+65%ï¼ˆ9.3%â†’15.3%ï¼‰ï¼ŒGRPO åè€Œä¸‹é™
   - CRUXEval-Oï¼š+76%ï¼ˆ23.6%â†’41.6%ï¼‰ï¼ŒGRPO ä»… +44%

7. **æ¶Œç°èƒ½åŠ›**ï¼š
   - é•¿é“¾æ¨ç†ã€æŒ‡ä»¤éµå¾ªï¼ˆAlpacaEval 3.72â†’7.10ï¼‰
   - **1.5B æ¨¡å‹å…³é”®**ï¼šä»ä¹±ç ï¼ˆ0%ï¼‰åˆ°è¿è´¯ä»£ç ï¼ˆ9.9%ï¼‰ï¼Œå­¦ä¼šæŒ‰ format è¾“å‡º

8. **ä¸ R1-Zero åŒºåˆ«**ï¼š
   - R1-Zeroï¼šç¬¬äºŒæ›²çº¿ï¼Œéœ€è¦é‡‘æ ‡ç­”æ¡ˆï¼ŒäºŒå…ƒå¥–åŠ±ï¼Œä»…é€‚ç”¨å¯éªŒè¯ä»»åŠ¡
   - Intuitorï¼šç¬¬ä¸‰æ›²çº¿ï¼Œä¸éœ€è¦é‡‘æ ‡ç­”æ¡ˆï¼Œè¿ç»­ token çº§å¥–åŠ±ï¼Œé€‚ç”¨ä»»æ„ä»»åŠ¡
   - R1-Zero çš„"Zero"æŒ‡é›¶ SFT æ•°æ®ï¼Œä½†ä»ä¾èµ–å¯éªŒè¯å¥–åŠ±

9. **æœºåˆ¶è§£é‡Š**ï¼š
   - ç½®ä¿¡åº¦ â†’ å†…éƒ¨ä¸€è‡´æ€§ â†’ è¿è´¯æ¨ç†é“¾
   - æ­£åé¦ˆå¾ªç¯ï¼šä¸ç¡®å®š â†’ è¯¦ç»†æ¨ç† â†’ æ›´ç†è§£ â†’ ç½®ä¿¡åº¦æå‡
   - ä»ç‰¹å®šåˆ°é€šç”¨ï¼šGRPO å­¦ç­”æ¡ˆæ¨¡å¼ï¼ŒIntuitor å­¦è¡¨è¾¾æ¨ç†

10. **æ·±å±‚æ„ä¹‰**ï¼š
    - AI è¶…è¶Šäººç±»æ—¶ï¼ŒRLIF æ˜¯å”¯ä¸€å¯è¡Œè·¯å¾„
    - ä¸º AGI è‡ªæˆ‘è¿›åŒ–æä¾›æ–¹æ³•è®º
    - ä¸äººç±»å­¦ä¹ ç±»ä¼¼ï¼ˆå‘è‡ªå·±è§£é‡Šï¼‰

æ‰€æœ‰å†…å®¹éƒ½åŸºäºé¡¹ç›® README çš„è¯¦ç»†æè¿°ï¼Œå‡†ç¡®åæ˜ äº† Intuitor çš„æ ¸å¿ƒæ€æƒ³ã€å®éªŒç»“æœå’Œæ·±å±‚æ„ä¹‰ã€‚

---

### ğŸ‘¤ You

æœ‰ä¸€éƒ¨åˆ†åº”è¯¥æŒªåˆ° è¶…è¶Šå¯éªŒè¯å¥–åŠ±ï¼šå†…åœ¨åé¦ˆä¸ç¬¬ä¸‰æ›²çº¿ çš„ç†è®ºéƒ¨åˆ†

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šå°†å…³äº"ä¸‰æ¡æ›²çº¿"çš„æ¡†æ¶æ€§å†…å®¹ä»å®éªŒ 7.10 æŒªåˆ°"è¶…è¶Šå¯éªŒè¯å¥–åŠ±ï¼šå†…åœ¨åé¦ˆä¸ç¬¬ä¸‰æ›²çº¿"çš„ç†è®ºéƒ¨åˆ†ã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»å°†å…³äº"ä¸‰æ¡æ›²çº¿"çš„æ¡†æ¶æ€§å†…å®¹ä»å®éªŒ 7.10 ç§»åˆ°äº†"è¶…è¶Šå¯éªŒè¯å¥–åŠ±ï¼šå†…åœ¨åé¦ˆä¸ç¬¬ä¸‰æ›²çº¿"çš„ç†è®ºéƒ¨åˆ†ã€‚ç°åœ¨çš„ç»“æ„æ›´åŠ æ¸…æ™°ï¼š

## è°ƒæ•´åçš„ç»“æ„ï¼š

**ç†è®ºéƒ¨åˆ†ï¼ˆ"è¶…è¶Šå¯éªŒè¯å¥–åŠ±ï¼šå†…åœ¨åé¦ˆä¸ç¬¬ä¸‰æ›²çº¿"ï¼‰**ç°åœ¨åŒ…å«ï¼š
1. å‰è¿° RL å®éªŒéƒ½ä¾èµ–å¯éªŒè¯å¤–éƒ¨å¥–åŠ±
2. å¤§é‡çœŸå®ä»»åŠ¡æ²¡æœ‰æ˜ç¡®å¯¹é”™æ ‡å‡†
3. **ä¸‰æ¡æ›²çº¿çš„å®Œæ•´æ¡†æ¶**ï¼š
   - ç¬¬ä¸€æ›²çº¿ï¼ˆé¢„è®­ç»ƒï¼‰ï¼šå­¦"æ˜¯ä»€ä¹ˆ"
   - ç¬¬äºŒæ›²çº¿ï¼ˆRLVRï¼‰ï¼šå­¦"å¯¹ä¸å¯¹"
   - ç¬¬ä¸‰æ›²çº¿ï¼ˆæ— ç›‘ç£ RLï¼‰ï¼šå­¦"å¥½ä¸å¥½"
4. ç¬¬ä¸‰æ›²çº¿çš„å„ç§æ–¹æ³•ï¼ˆå†…éƒ¨åé¦ˆã€ä¸€è‡´æ€§ã€è§„åˆ™å¥–åŠ±ç­‰ï¼‰
5. ä¸º"AI è¶…è¶Šäººç±»ç›‘ç£"æä¾›æ–¹æ³•è®º

**å®éªŒ 7.10** ç°åœ¨ä¸“æ³¨äº Intuitor çš„å…·ä½“å®ç°ï¼š
1. Self-certainty çš„æ•°å­¦å®šä¹‰
2. å®éªŒè®¾ç½®ï¼ˆQwen2.5-3Bï¼ŒMATHï¼ŒGRPO æ¡†æ¶ï¼‰
3. å…·ä½“ç»“æœï¼ˆåŸŸå†…ã€åŸŸå¤–å¯¹æ¯”ï¼‰
4. æ¶Œç°èƒ½åŠ›
5. ä¸ R1-Zero çš„åŒºåˆ«
6. æœºåˆ¶è§£é‡Š
7. æ·±å±‚æ„ä¹‰

è¿™æ ·çš„ç»“æ„æ›´ç¬¦åˆå­¦æœ¯å†™ä½œçš„é€»è¾‘ï¼šå…ˆåœ¨ç†è®ºéƒ¨åˆ†å»ºç«‹å®Œæ•´çš„æ¦‚å¿µæ¡†æ¶ï¼Œå†åœ¨å®éªŒéƒ¨åˆ†è¯¦ç»†å±•å¼€å…·ä½“å®ç°å’Œç»“æœã€‚

---

### ğŸ‘¤ You

æ·±å±‚æ„ä¹‰å’Œè¶…è¶Šå¯éªŒè¯å¥–åŠ±çš„ç†è®ºéƒ¨åˆ†ï¼Œä½¿ç”¨ OpenAI çš„äº”ç­‰çº§ç†è®ºæ¥ä»‹ç»ï¼Œç¬¬ä¸€çº§ä¸»è¦é é¢„è®­ç»ƒï¼Œç¬¬äºŒã€ä¸‰çº§ä¸»è¦é  RLï¼ˆæœ¬ç« çš„é‡ç‚¹ï¼‰ï¼Œè¦è¾¾åˆ°ç¬¬å››çº§å°±å¿…é¡»è¶…è¶Šå¯éªŒè¯å¥–åŠ±

Â· Level 1 Stage AI: Chatbots, AI with conversational language

Â· Level 2 Stage AI: Reasoners, human-level problem solving

Â· Level 3 Stage AI: Agents, systems that can take actions

Â· Level 4 Stage AI: Innovators, AI that can aid in invention

Â· Level 5 Stage AI: Organizations: AI that can do the work of an organization

---

### ğŸ‘¤ You

æ·±å±‚æ„ä¹‰å’Œè¶…è¶Šå¯éªŒè¯å¥–åŠ±çš„ç†è®ºéƒ¨åˆ†ï¼Œä¸è¦ç”¨ç¬¬ä¸‰æ›²çº¿ï¼Œæ”¹æˆä½¿ç”¨ OpenAI çš„äº”ç­‰çº§ç†è®ºæ¥ä»‹ç»ï¼Œç¬¬ä¸€çº§ä¸»è¦é é¢„è®­ç»ƒï¼Œç¬¬äºŒã€ä¸‰çº§ä¸»è¦é  RLï¼ˆæœ¬ç« çš„é‡ç‚¹ï¼‰ï¼Œè¦è¾¾åˆ°ç¬¬å››çº§å°±å¿…é¡»è¶…è¶Šå¯éªŒè¯å¥–åŠ±

Â· Level 1 Stage AI: Chatbots, AI with conversational language

Â· Level 2 Stage AI: Reasoners, human-level problem solving

Â· Level 3 Stage AI: Agents, systems that can take actions

Â· Level 4 Stage AI: Innovators, AI that can aid in invention

Â· Level 5 Stage AI: Organizations: AI that can do the work of an organization

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šç”¨ OpenAI çš„äº”ç­‰çº§ç†è®ºæ›¿æ¢"ç¬¬ä¸‰æ›²çº¿"çš„æ¡†æ¶ï¼Œé‡æ–°ç»„ç»‡ç†è®ºéƒ¨åˆ†å’Œå®éªŒ 7.10 çš„æ·±å±‚æ„ä¹‰ã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»ç”¨ OpenAI çš„äº”ç­‰çº§ç†è®ºæ›¿æ¢äº†"ç¬¬ä¸‰æ›²çº¿"æ¡†æ¶ï¼Œä¸»è¦æ”¹åŠ¨ï¼š

## ç†è®ºéƒ¨åˆ†ï¼ˆ"è¶…è¶Šå¯éªŒè¯å¥–åŠ±ï¼šä» Level 3 åˆ° Level 4 çš„è·¨è¶Š"ï¼‰ï¼š

1. **äº”ç­‰çº§ç†è®º**ï¼š
   - Level 1ï¼šå¯¹è¯æœºå™¨äººï¼ˆä¸»è¦é é¢„è®­ç»ƒï¼‰
   - Level 2ï¼šæ¨ç†è€…ï¼ˆäººç±»æ°´å¹³é—®é¢˜æ±‚è§£ï¼‰
   - Level 3ï¼šæ™ºèƒ½ä½“ï¼ˆèƒ½é‡‡å–è¡ŒåŠ¨ï¼‰
   - **Level 4ï¼šåˆ›æ–°è€…ï¼ˆè¾…åŠ©å‘æ˜åˆ›é€ ï¼‰**
   - Level 5ï¼šç»„ç»‡ï¼ˆç»„ç»‡çº§å·¥ä½œï¼‰

2. **æœ¬ç« é‡ç‚¹**ï¼šLevel 2 ä¸ Level 3ï¼Œé€šè¿‡å¯éªŒè¯å¥–åŠ± RL æå‡èƒ½åŠ›

3. **Level 4 çš„æŒ‘æˆ˜**ï¼š
   - åˆ›æ–°æ¢ç´¢æœªçŸ¥ï¼Œæ— é¢„å®šä¹‰"æ­£ç¡®ç­”æ¡ˆ"
   - ç»„ç»‡çº§å†³ç­–æ¶‰åŠå¤šç›®æ ‡æƒè¡¡ã€ä¸»è§‚åˆ¤æ–­
   - AI è¶…è¶Šäººç±»æ—¶ï¼Œå¤–éƒ¨ç›‘ç£ä¸å¯è¡Œ

4. **æ— ç›‘ç£ RL çš„å¿…è¦æ€§**ï¼šä¸ºä» Level 3 åˆ° Level 4 æä¾›æ–¹æ³•è®º

## å®éªŒ 7.10 è°ƒæ•´ï¼š

1. æ ‡é¢˜æ”¹ä¸º"å†…åœ¨åé¦ˆé©±åŠ¨çš„è‡ªæˆ‘æå‡"ï¼ˆå»æ‰"ç¬¬ä¸‰æ›²çº¿"ï¼‰
2. æ·±å±‚æ„ä¹‰æ”¹ä¸ºï¼šä¸ºä» Level 3 åˆ° Level 4 çš„è·¨è¶Šæä¾›æ–¹æ³•è®ºæ¢ç´¢
3. å¼ºè°ƒï¼šLevel 2/3 å¯ç”¨å¯éªŒè¯å¥–åŠ±ï¼ŒLevel 4/5 éœ€è¦å†…åœ¨åé¦ˆ
4. è¿™ç§è‡ªæˆ‘æå‡èƒ½åŠ›æ˜¯é€šå¾€æ›´é«˜ç­‰çº§ AI çš„å¿…è¦åŸºç¡€

ç°åœ¨çš„æ¡†æ¶æ›´åŠ æ¸…æ™°ï¼Œä¸ OpenAI çš„å®˜æ–¹ç†è®ºä¸€è‡´ï¼ŒåŒæ—¶ä¿æŒäº†å­¦æœ¯ä¸¥è°¨æ€§ã€‚

---

### ğŸ‘¤ You

å†æ”¹è¿›ä¸€ä¸‹ï¼Œäººç±»ç§‘å­¦å²ä¸å°±æ˜¯ä¸æ–­å¯»æ‰¾æ›´ç®€æ´å’Œè‡ªæ´½çš„æ¨¡å‹æ¥è§£é‡Šè§‚å¯Ÿåˆ°çš„ç°è±¡å—ï¼Ÿå¦‚æœ AI èƒ½å¤Ÿå­¦ä¼šä»è§‚å¯Ÿåˆ°çš„ç°è±¡è‡ªä¸»æç‚¼è§„å¾‹ï¼Œç”¨è‡ªæ´½çš„æ¨¡å‹è§£é‡Šè¿™äº›ç°è±¡ï¼Œå°±å…·å¤‡äº†è‡ªä¸»ç ”ç©¶å’Œåˆ›æ–°çš„èƒ½åŠ›

---

### ğŸ¤– Cursor Assistant

æˆ‘ä¼šæ”¹è¿›è¿™éƒ¨åˆ†å†…å®¹ï¼ŒåŠ å…¥ç§‘å­¦å²çš„ç±»æ¯”ï¼Œè¯´æ˜å†…åœ¨ä¸€è‡´æ€§ä¸ç§‘å­¦åˆ›æ–°çš„æ·±å±‚è”ç³»ã€‚

---

### ğŸ¤– Cursor Assistant

å®Œæˆï¼æˆ‘å·²ç»æ”¹è¿›äº†å®éªŒ 7.10 çš„æ·±å±‚æ„ä¹‰éƒ¨åˆ†ï¼ŒåŠ å…¥äº†ç§‘å­¦å²çš„ç±»æ¯”ï¼š

## ä¸»è¦æ”¹è¿›ï¼š

1. **ç§‘å­¦å²çš„ç±»æ¯”**ï¼š
   - ç§‘å­¦è¿›æ­¥ = å¯»æ‰¾æ›´ç®€æ´ã€æ›´è‡ªæ´½çš„æ¨¡å‹è§£é‡Šç°è±¡
   - ä¾‹å­ï¼šæ‰˜å‹’å¯† â†’ å“¥ç™½å°¼ï¼Œç‰›é¡¿ â†’ çˆ±å› æ–¯å¦
   - è¿½æ±‚"ç”¨æ›´å°‘å‡è®¾ã€æ›´ç»Ÿä¸€åŸç†è§£é‡Šæ›´å¤šç°è±¡"

2. **ç§‘å­¦æ–¹æ³•è®ºçš„æœ¬è´¨**ï¼š
   - åˆ¤æ–­æ ‡å‡†ä¸æ˜¯å¤–éƒ¨æƒå¨éªŒè¯
   - è€Œæ˜¯ç†è®ºçš„å†…åœ¨è‡ªæ´½æ€§ã€è§£é‡ŠåŠ›ä¸ç®€æ´æ€§

3. **ä¸ AI åˆ›æ–°çš„è”ç³»**ï¼š
   - AI å­¦ä¼šä»è§‚å¯Ÿæç‚¼è§„å¾‹
   - ç”¨è‡ªæ´½æ¨¡å‹è§£é‡Šç°è±¡
   - å°±å…·å¤‡äº†è‡ªä¸»ç ”ç©¶å’Œåˆ›æ–°çš„åŸºç¡€èƒ½åŠ›

4. **Intuitor çš„å½¢å¼åŒ–**ï¼š
   - Self-certainty ä¼˜åŒ– = ç§‘å­¦æ–¹æ³•è®ºçš„å½¢å¼åŒ–
   - ä¼˜åŒ–"å¯¹æ¨ç†çš„ç¡®ä¿¡ç¨‹åº¦" = æ„å»ºå†…éƒ¨è‡ªæ´½çš„è§£é‡Šä½“ç³»
   - é«˜ç¡®ä¿¡ = è¿è´¯ã€æ— çŸ›ç›¾ã€å¯è¢«è‡ªå·±"ç†è§£"

5. **ä¸ç§‘å­¦å®¶çš„ç±»æ¯”**ï¼š
   - ç§‘å­¦å®¶çš„ä¿¡å¿ƒæ¥æºï¼šé€»è¾‘è‡ªæ´½ã€ç»éªŒä¸€è‡´
   - ä¸æ˜¯å› ä¸ºæƒå¨è¯´å¯¹ï¼Œè€Œæ˜¯å› ä¸ºå†…åœ¨ä¸€è‡´

è¿™æ ·çš„æ”¹è¿›å°† Intuitor çš„æŠ€æœ¯åˆ›æ–°ä¸äººç±»ç§‘å­¦å‘ç°çš„æœ¬è´¨è”ç³»èµ·æ¥ï¼Œä¸ºç†è§£"å†…åœ¨åé¦ˆå¦‚ä½•é©±åŠ¨åˆ›æ–°"æä¾›äº†æ·±åˆ»çš„å“²å­¦ä¸å†å²è§†è§’ï¼Œä½¿è¯»è€…èƒ½å¤Ÿç†è§£ä¸ºä»€ä¹ˆå†…åœ¨ä¸€è‡´æ€§ä¼˜åŒ–æ˜¯é€šå¾€ Level 4ï¼ˆåˆ›æ–°è€…ï¼‰çš„å…³é”®è·¯å¾„ã€‚

---


*Exported from [Cursor View](https://github.com/saharmor/cursor-view)*